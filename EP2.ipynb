{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a890932b",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Instalação de Dependências e Hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4c029e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (4.57.3)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.12/site-packages (4.4.1)\n",
      "Requirement already satisfied: evaluate in ./.venv/lib/python3.12/site-packages (0.4.6)\n",
      "Requirement already satisfied: sacrebleu in ./.venv/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: openpyxl in ./.venv/lib/python3.12/site-packages (3.1.5)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.9.1+cpu)\n",
      "Requirement already satisfied: peft in ./.venv/lib/python3.12/site-packages (0.18.0)\n",
      "Requirement already satisfied: sentencepiece in ./.venv/lib/python3.12/site-packages (0.2.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.12/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.venv/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./.venv/lib/python3.12/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in ./.venv/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in ./.venv/lib/python3.12/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: portalocker in ./.venv/lib/python3.12/site-packages (from sacrebleu) (3.2.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in ./.venv/lib/python3.12/site-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: colorama in ./.venv/lib/python3.12/site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in ./.venv/lib/python3.12/site-packages (from sacrebleu) (6.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: et-xmlfile in ./.venv/lib/python3.12/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (70.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from peft) (7.1.3)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in ./.venv/lib/python3.12/site-packages (from peft) (1.12.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "# Instalação das dependências\n",
    "!pip install transformers datasets evaluate sacrebleu pandas openpyxl torch peft sentencepiece\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import (\n",
    "    MBart50TokenizerFast,\n",
    "    MBartForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "preferred_device = \"cuda\"\n",
    "device = torch.device(preferred_device if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70b58017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir diretório raiz do projeto\n",
    "PROJECT_ROOT_DIR = \"/home/biel/Documentos/projetos-de-software/ep2/EP2-mac0508\"\n",
    "\n",
    "# Se estiver usando colab use \n",
    "# PROJECT_ROOT_DIR = \"/content/drive/MyDrive/EP2-mac0508\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30f6e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "\n",
    "    \"model_checkpoint\": \"facebook/mbart-large-50-many-to-many-mmt\",\n",
    "    \"nllb_checkpoint\": \"facebook/nllb-200-distilled-600M\",\n",
    "\n",
    "    \"max_input_length\": 64,\n",
    "    \"max_target_length\": 64,\n",
    "\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"batch_size\": 8,\n",
    "    \"num_epochs\": 6,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_ratio\": 0.05,\n",
    "\n",
    "    \"early_stopping_patience\": 3,\n",
    "\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "\n",
    "    \"project_root_dir\": PROJECT_ROOT_DIR,\n",
    "    \"data_path\": os.path.join(PROJECT_ROOT_DIR, \"data.xlsx\"),\n",
    "    \"processed_data_dir\": os.path.join(PROJECT_ROOT_DIR, \"processed_data\"),\n",
    "    \"results_dir\": os.path.join(PROJECT_ROOT_DIR, \"results\"),\n",
    "    \"models_dir\": os.path.join(PROJECT_ROOT_DIR, \"models\"),\n",
    "\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG[\"results_dir\"], exist_ok=True)\n",
    "os.makedirs(CONFIG[\"models_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a1f64a",
   "metadata": {},
   "source": [
    "## 2. Carregamento e Preparação dos Dados\n",
    "\n",
    "### 2.1 Leitura do Corpus\n",
    "\n",
    "corpus Português/TupiAntigo em `data.xlsx`\n",
    "\n",
    "**Observacao**: Preservei acentos, diacríticos e grafia histórica do Tupi Antigo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "031e902d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus carregado: 7097 pares de frases\n",
      "\n",
      "Colunas: ['Português', 'Tupi Antigo']\n",
      "\n",
      "Primeiras 5 linhas:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Português</th>\n",
       "      <th>Tupi Antigo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aparei as pontas deles</td>\n",
       "      <td>Aîapyr-etab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Doravante assim procedo</td>\n",
       "      <td>Ko'yré emonã aîkó</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As doenças da alma do homem com ele saram bem</td>\n",
       "      <td>Abá 'anga mara'ara i pupé opûeîrá-katu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>É discreta, falando aos homens</td>\n",
       "      <td>I kunusãî abá supé onhe'enga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Para se vingar de seu cão que cria, um homem n...</td>\n",
       "      <td>O eîmbaba îagûara resé oîepyka, abá n'oîmomba'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Português  \\\n",
       "0                             Aparei as pontas deles   \n",
       "1                            Doravante assim procedo   \n",
       "2      As doenças da alma do homem com ele saram bem   \n",
       "3                     É discreta, falando aos homens   \n",
       "4  Para se vingar de seu cão que cria, um homem n...   \n",
       "\n",
       "                                         Tupi Antigo  \n",
       "0                                        Aîapyr-etab  \n",
       "1                                  Ko'yré emonã aîkó  \n",
       "2             Abá 'anga mara'ara i pupé opûeîrá-katu  \n",
       "3                       I kunusãî abá supé onhe'enga  \n",
       "4  O eîmbaba îagûara resé oîepyka, abá n'oîmomba'...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(CONFIG[\"data_path\"])\n",
    "\n",
    "df.columns = df.columns.str.replace('PortuguÊs', 'Português', regex=False)\n",
    "\n",
    "print(f\"Corpus carregado: {len(df)} pares de frases\")\n",
    "print(f\"\\nColunas: {list(df.columns)}\")\n",
    "print(f\"\\nPrimeiras 5 linhas:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f8a8a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            len_pt       len_ta\n",
      "count  7097.000000  7097.000000\n",
      "mean     35.728054    26.222911\n",
      "std      21.140072    17.144765\n",
      "min       3.000000     2.000000\n",
      "25%      20.000000    13.000000\n",
      "50%      31.000000    21.000000\n",
      "75%      47.000000    36.000000\n",
      "max     187.000000   136.000000\n"
     ]
    }
   ],
   "source": [
    "df['len_pt'] = df['Português'].astype(str).str.len()\n",
    "df['len_ta'] = df['Tupi Antigo'].astype(str).str.len()\n",
    "print(df[['len_pt', 'len_ta']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af7ec89",
   "metadata": {},
   "source": [
    "### 2.2 Limpeza e Normalização\n",
    "\n",
    "Realizei limpeza dos dados conforme orientação do professor:\n",
    "\n",
    "#### Português:\n",
    "1. **Remoção de expressões entre parênteses**: O corpus contém anotações explicativas entre parênteses que não aparecem na tradução Tupi\n",
    "2. **Remoção de espaços extras** (início, fim, múltiplos espaços)\n",
    "3. **Remoção de caracteres invisíveis**\n",
    "\n",
    "#### Texto em Tupi Antigo:\n",
    "1. **Apenas limpeza básica** - preservei acentos, diacríticos e grafia histórica\n",
    "2. **NÃO removido**: acentos, diacríticos, maiúsculas/minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "453d6f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Após limpeza: 7097 frases\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    text = re.sub(r'[\\u200b\\u200c\\u200d\\ufeff]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_portuguese_text(text):\n",
    "\n",
    "    text = clean_text(text)\n",
    "    text = re.sub(r'\\s*\\([^)]*\\)\\s*', ' ', text)\n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    text = re.sub(r',\\s*,', ',', text)\n",
    "    text = re.sub(r',\\s*$', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "df['Português'] = df['Português'].apply(clean_portuguese_text)\n",
    "df['Tupi Antigo'] = df['Tupi Antigo'].apply(clean_text)\n",
    "df = df[(df['Português'] != '') & (df['Tupi Antigo'] != '')]\n",
    "\n",
    "df = df.drop(columns=['len_pt', 'len_ta'], errors='ignore')\n",
    "\n",
    "print(f\"Após limpeza: {len(df)} frases\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef89123c",
   "metadata": {},
   "source": [
    "### 2.3 Divisão do Corpus\n",
    "\n",
    "Dividi corpus em três conjuntos:\n",
    "- **Treino (70%)**: Para fine-tuning do modelo\n",
    "- **Validação (15%)**: Para early stopping e seleção de hiperparâmetros\n",
    "- **Teste (15%)**: Para avaliação final (nunca usado durante treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89c8f07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divisão do corpus:\n",
      "  Treino:     4967 (70.0%)\n",
      "  Validação:  1065 (15.0%)\n",
      "  Teste:      1065 (15.0%)\n",
      "  Total:      7097\n",
      "Subconjuntos salvos em ./data/\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(CONFIG[\"seed\"])\n",
    "train_df, temp_df = train_test_split(df, test_size=0.30, random_state=CONFIG[\"seed\"])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.50, random_state=CONFIG[\"seed\"])\n",
    "\n",
    "print(f\"Divisão do corpus:\")\n",
    "print(f\"  Treino:     {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Validação:  {len(val_df)} ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Teste:      {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Total:      {len(df)}\")\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "\n",
    "train_df.to_csv(\"./data/train.csv\", index=False)\n",
    "val_df.to_csv(\"./data/val.csv\", index=False)\n",
    "test_df.to_csv(\"./data/test.csv\", index=False)\n",
    "\n",
    "print(\"Subconjuntos salvos em ./data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97f54234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['portuguese', 'tupi'],\n",
      "        num_rows: 4967\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['portuguese', 'tupi'],\n",
      "        num_rows: 1065\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['portuguese', 'tupi'],\n",
      "        num_rows: 1065\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def df_to_dataset(df):\n",
    "    return Dataset.from_dict({\n",
    "        \"portuguese\": df[\"Português\"].tolist(),\n",
    "        \"tupi\": df[\"Tupi Antigo\"].tolist()\n",
    "    })\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": df_to_dataset(train_df),\n",
    "    \"validation\": df_to_dataset(val_df),\n",
    "    \"test\": df_to_dataset(test_df)\n",
    "})\n",
    "\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3140e7a",
   "metadata": {},
   "source": [
    "## 3. Modelo Tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a918f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MBart50TokenizerFast.from_pretrained(CONFIG[\"model_checkpoint\"])\n",
    "\n",
    "LANG_CODE = \"pt_XX\"\n",
    "\n",
    "PREFIX_PT_TO_TA = \"traduzir para tupi: \"\n",
    "PREFIX_TA_TO_PT = \"traduzir para português: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6528a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pt_to_ta(examples):\n",
    "\n",
    "    inputs = examples[\"portuguese\"]\n",
    "    targets = examples[\"tupi\"]\n",
    "    \n",
    "    tokenizer.src_lang = LANG_CODE\n",
    "    tokenizer.tgt_lang = LANG_CODE\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=CONFIG[\"max_input_length\"], \n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    labels = tokenizer(\n",
    "        text_target=targets,\n",
    "        max_length=CONFIG[\"max_target_length\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_ta_to_pt(examples):\n",
    "    \"\"\"Pré-processa para tradução Tupi Antigo -> Português.\"\"\"\n",
    "    inputs = examples[\"tupi\"]\n",
    "    targets = examples[\"portuguese\"]\n",
    "    \n",
    "    tokenizer.src_lang = LANG_CODE\n",
    "    tokenizer.tgt_lang = LANG_CODE\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=CONFIG[\"max_input_length\"], \n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    labels = tokenizer(\n",
    "        text_target=targets,\n",
    "        max_length=CONFIG[\"max_target_length\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953c99a9",
   "metadata": {},
   "source": [
    "## 4. Configuração das Métricas de Avaliação\n",
    "\n",
    "Métricas implementadas conforme especificado no enunciado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e66d21a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67223e50d8a4260b2cb38f499e9c75e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8cf2a65d6884a4ca0a897b5dfe433f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "chrf_metric = evaluate.load(\"chrf\")\n",
    "\n",
    "def compute_all_metrics(predictions, references):\n",
    "\n",
    "    refs_for_bleu = [[ref] for ref in references]\n",
    "    bleu_result = bleu_metric.compute(predictions=predictions, references=refs_for_bleu)\n",
    "    \n",
    "    chrf1_result = chrf_metric.compute(\n",
    "        predictions=predictions, \n",
    "        references=refs_for_bleu,\n",
    "        char_order=6,\n",
    "        word_order=0,\n",
    "        beta=1\n",
    "    )\n",
    "\n",
    "    chrf3_result = chrf_metric.compute(\n",
    "        predictions=predictions, \n",
    "        references=refs_for_bleu,\n",
    "        char_order=6,\n",
    "        word_order=0,\n",
    "        beta=3\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"bleu\": bleu_result[\"score\"],\n",
    "        \"chrf1\": chrf1_result[\"score\"],\n",
    "        \"chrf3\": chrf3_result[\"score\"],\n",
    "        \"bleu_details\": {\n",
    "            \"precisions\": bleu_result[\"precisions\"],\n",
    "            \"brevity_penalty\": bleu_result[\"bp\"],\n",
    "            \"length_ratio\": bleu_result[\"sys_len\"] / bleu_result[\"ref_len\"] if bleu_result[\"ref_len\"] > 0 else 0\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e82aa9",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Zero-Shot\n",
    "\n",
    "\n",
    "### Usar NLLB com Guarani como Proxy\n",
    "\n",
    "Conforme orientação do professor:\n",
    "- O **mBART com pt→pt** produz tradução quase nula no zero-shot\n",
    "- O modelo **NLLB-200** possui suporte ao **Guarani** (`grn_Latn`), língua da família Tupi-Guarani relacionada ao Tupi Antigo\n",
    "- Usamos Guarani como língua proxy para aproximar o Tupi Antigo\n",
    "\n",
    "### Transformação Guarani → Tupi Antigo\n",
    "\n",
    "Para melhorar a correspondência entre a saída do modelo (Guarani) e o Tupi Antigo de referência, apliquei transformações básicas conforme sugerido pelo professor:\n",
    "- `ñ` → `nh` (nasalização)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fb78c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "NLLB_CHECKPOINT = \"facebook/nllb-200-distilled-600M\"\n",
    "\n",
    "print(\"Carregando modelo NLLB-200 para zero-shot...\")\n",
    "print(\"(NLLB possui Guarani - língua da família Tupi-Guarani)\")\n",
    "\n",
    "tokenizer_nllb = AutoTokenizer.from_pretrained(NLLB_CHECKPOINT)\n",
    "model_zero_shot = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    NLLB_CHECKPOINT,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "model_zero_shot = model_zero_shot.to(device)\n",
    "model_zero_shot.eval()\n",
    "\n",
    "LANG_PT_NLLB = \"por_Latn\"\n",
    "LANG_GN_NLLB = \"grn_Latn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd27d6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_batch_nllb(model, tokenizer, texts, source_lang, target_lang, batch_size=8, \n",
    "                          num_beams=5, max_length=128):\n",
    "    model.eval()\n",
    "    translations = []\n",
    "    \n",
    "    tokenizer.src_lang = source_lang\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            batch, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(\n",
    "                **inputs,\n",
    "                forced_bos_token_id=tokenizer.convert_tokens_to_ids(target_lang),\n",
    "                max_length=max_length,\n",
    "                num_beams=num_beams,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        decoded = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "        translations.extend(decoded)\n",
    "        \n",
    "        if (i // batch_size + 1) % 20 == 0:\n",
    "            print(f\"  Traduzidos: {min(i + batch_size, len(texts))}/{len(texts)}\")\n",
    "    \n",
    "    return translations\n",
    "\n",
    "def guarani_to_tupi_transform(text):\n",
    "\n",
    "    text = text.replace('ñ', 'nh')\n",
    "    text = text.replace('Ñ', 'Nh')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c9266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_batch(model, texts, source_lang, target_lang, batch_size=8):\n",
    "    model.eval()\n",
    "    translations = []\n",
    "    \n",
    "    tokenizer.src_lang = source_lang\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            batch, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True,\n",
    "            max_length=CONFIG[\"max_input_length\"]\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(\n",
    "                **inputs,\n",
    "                forced_bos_token_id=tokenizer.lang_code_to_id[target_lang],\n",
    "                max_length=CONFIG[\"max_target_length\"],\n",
    "                num_beams=5,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        decoded = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "        translations.extend(decoded)\n",
    "        \n",
    "        if (i // batch_size + 1) % 10 == 0:\n",
    "            print(f\"  Traduzidos: {min(i + batch_size, len(texts))}/{len(texts)}\")\n",
    "    \n",
    "    return translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11cb99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_portuguese = test_df[\"Português\"].tolist()\n",
    "test_tupi_ref = test_df[\"Tupi Antigo\"].tolist()\n",
    "test_tupi = test_df[\"Tupi Antigo\"].tolist()\n",
    "test_portuguese_ref = test_df[\"Português\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5cc0b3",
   "metadata": {},
   "source": [
    "### 5.1 Zero-Shot: Português para Tupi Antigo (via Guarani)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ee9801",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ZERO-SHOT: Português → Tupi Antigo (via NLLB + Guarani)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nTraduzindo PT → Guarani...\")\n",
    "translations_pt_gn = translate_batch_nllb(\n",
    "    model_zero_shot,\n",
    "    tokenizer_nllb,\n",
    "    test_portuguese,\n",
    "    source_lang=LANG_PT_NLLB,\n",
    "    target_lang=LANG_GN_NLLB,\n",
    "    num_beams=5,\n",
    "    max_length=CONFIG[\"max_target_length\"]\n",
    ")\n",
    "\n",
    "print(\"Aplicando transformação Guarani → Tupi Antigo (ñ → nh)...\")\n",
    "translations_pt_ta_zero = [guarani_to_tupi_transform(t) for t in translations_pt_gn]\n",
    "\n",
    "metrics_pt_ta_zero = compute_all_metrics(translations_pt_ta_zero, test_tupi_ref)\n",
    "\n",
    "print(f\"\\nTraduzidas {len(translations_pt_ta_zero)} frases.\")\n",
    "print(\"\\n=== Métricas Zero-Shot PT → TA (via Guarani) ===\")\n",
    "print(f\"  BLEU:  {metrics_pt_ta_zero['bleu']:.2f}\")\n",
    "print(f\"  chrF1: {metrics_pt_ta_zero['chrf1']:.2f}\")\n",
    "print(f\"  chrF3: {metrics_pt_ta_zero['chrf3']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bbc75a",
   "metadata": {},
   "source": [
    "### 5.2 Zero-Shot: Tupi Antigo para Português (via Guarani)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f38eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ZERO-SHOT: Tupi Antigo → Português (via NLLB + Guarani)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nTraduzindo Tupi (como Guarani) → PT...\")\n",
    "translations_ta_pt_zero = translate_batch_nllb(\n",
    "    model_zero_shot,\n",
    "    tokenizer_nllb,\n",
    "    test_tupi,\n",
    "    source_lang=LANG_GN_NLLB,\n",
    "    target_lang=LANG_PT_NLLB,\n",
    "    num_beams=5,\n",
    "    max_length=CONFIG[\"max_target_length\"]\n",
    ")\n",
    "\n",
    "metrics_ta_pt_zero = compute_all_metrics(translations_ta_pt_zero, test_portuguese_ref)\n",
    "\n",
    "print(f\"\\nTraduzidas {len(translations_ta_pt_zero)} frases.\")\n",
    "print(\"\\n=== Métricas Zero-Shot TA → PT (via Guarani) ===\")\n",
    "print(f\"  BLEU:  {metrics_ta_pt_zero['bleu']:.2f}\")\n",
    "print(f\"  chrF1: {metrics_ta_pt_zero['chrf1']:.2f}\")\n",
    "print(f\"  chrF3: {metrics_ta_pt_zero['chrf3']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28725fc3",
   "metadata": {},
   "source": [
    "### 5.3 Salva Resultados Zero-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237517e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_zero_shot = {\n",
    "    \"pt_to_ta\": {\n",
    "        \"bleu\": metrics_pt_ta_zero[\"bleu\"],\n",
    "        \"chrf1\": metrics_pt_ta_zero[\"chrf1\"],\n",
    "        \"chrf3\": metrics_pt_ta_zero[\"chrf3\"],\n",
    "        \"bleu_details\": metrics_pt_ta_zero[\"bleu_details\"]\n",
    "    },\n",
    "    \"ta_to_pt\": {\n",
    "        \"bleu\": metrics_ta_pt_zero[\"bleu\"],\n",
    "        \"chrf1\": metrics_ta_pt_zero[\"chrf1\"],\n",
    "        \"chrf3\": metrics_ta_pt_zero[\"chrf3\"],\n",
    "        \"bleu_details\": metrics_ta_pt_zero[\"bleu_details\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(CONFIG[\"results_dir\"], \"results_zero_shot.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_zero_shot, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "os.makedirs(os.path.join(CONFIG[\"results_dir\"], \"outputs_zero_shot\"), exist_ok=True)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"source\": test_portuguese,\n",
    "    \"reference\": test_tupi_ref,\n",
    "    \"translation\": translations_pt_ta_zero\n",
    "}).to_csv(os.path.join(CONFIG[\"results_dir\"], \"outputs_zero_shot\", \"pt_to_ta.csv\"), index=False)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"source\": test_tupi,\n",
    "    \"reference\": test_portuguese_ref,\n",
    "    \"translation\": translations_ta_pt_zero\n",
    "}).to_csv(os.path.join(CONFIG[\"results_dir\"], \"outputs_zero_shot\", \"ta_to_pt.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db067c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liberar memória do modelo zero-shot\n",
    "del model_zero_shot\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"Memória liberada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c740bd99",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Fine-Tuning (Few-Shot Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4740457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainer(model, tokenizer, train_dataset, val_dataset, output_dir, direction):\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "    \n",
    "    def compute_metrics_trainer(eval_preds):\n",
    "        preds, labels = eval_preds\n",
    "        \n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "        \n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        \n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        refs = [[l] for l in decoded_labels]\n",
    "        bleu_result = bleu_metric.compute(predictions=decoded_preds, references=refs)\n",
    "        \n",
    "        return {\"bleu\": bleu_result[\"score\"]}\n",
    "    \n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=CONFIG[\"learning_rate\"],\n",
    "        per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "        per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "        num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "        weight_decay=CONFIG[\"weight_decay\"],\n",
    "        warmup_ratio=CONFIG[\"warmup_ratio\"],\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"bleu\",\n",
    "        greater_is_better=True,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=CONFIG[\"max_target_length\"],\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        logging_steps=50,\n",
    "        save_total_limit=2,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics_trainer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=CONFIG[\"early_stopping_patience\"])]\n",
    "    )\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0509dd62",
   "metadata": {},
   "source": [
    "### 6.1 Fine-Tuning: Português para Tupi Antigo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f704e16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_pt_ta = dataset_dict[\"train\"].map(preprocess_pt_to_ta, batched=True)\n",
    "tokenized_val_pt_ta = dataset_dict[\"validation\"].map(preprocess_pt_to_ta, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd9c8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pt_ta = MBartForConditionalGeneration.from_pretrained(\n",
    "    CONFIG[\"model_checkpoint\"],\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "model_pt_ta = get_peft_model(model_pt_ta, lora_config)\n",
    "model_pt_ta.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628dd702",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Treinamento PT → TA...\")\n",
    "trainer_pt_ta = create_trainer(\n",
    "    model=model_pt_ta,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_train_pt_ta,\n",
    "    val_dataset=tokenized_val_pt_ta,\n",
    "    output_dir=os.path.join(CONFIG[\"models_dir\"], \"pt_to_ta\"),\n",
    "    direction=\"pt_to_ta\"\n",
    ")\n",
    "\n",
    "train_result_pt_ta = trainer_pt_ta.train()\n",
    "\n",
    "print(\"\\n=== Treinamento PT → TA Concluído ===\")\n",
    "print(f\"Épocas: {train_result_pt_ta.metrics.get('epoch', 'N/A')}\")\n",
    "print(f\"Loss final: {train_result_pt_ta.metrics.get('train_loss', 'N/A'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fa2a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pt_ta.save_pretrained(os.path.join(CONFIG[\"models_dir\"], \"pt_to_ta_final\"))\n",
    "tokenizer.save_pretrained(os.path.join(CONFIG[\"models_dir\"], \"pt_to_ta_final\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f06c6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libera memoria do modelo PT -> TA\n",
    "del model_pt_ta, trainer_pt_ta\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe78c44",
   "metadata": {},
   "source": [
    "### 6.2 Fine-Tuning: Tupi Antigo para Português"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8fa293",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_ta_pt = dataset_dict[\"train\"].map(preprocess_ta_to_pt, batched=True)\n",
    "tokenized_val_ta_pt = dataset_dict[\"validation\"].map(preprocess_ta_to_pt, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7557fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ta_pt = MBartForConditionalGeneration.from_pretrained(\n",
    "    CONFIG[\"model_checkpoint\"],\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "\n",
    "model_ta_pt = get_peft_model(model_ta_pt, lora_config)\n",
    "model_ta_pt.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0042f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Treinamento TA → PT...\")\n",
    "trainer_ta_pt = create_trainer(\n",
    "    model=model_ta_pt,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_train_ta_pt,\n",
    "    val_dataset=tokenized_val_ta_pt,\n",
    "    output_dir=os.path.join(CONFIG[\"models_dir\"], \"ta_to_pt\"),\n",
    "    direction=\"ta_to_pt\"\n",
    ")\n",
    "\n",
    "train_result_ta_pt = trainer_ta_pt.train()\n",
    "\n",
    "print(\"\\n=== Treinamento TA → PT Concluído ===\")\n",
    "print(f\"Épocas: {train_result_ta_pt.metrics.get('epoch', 'N/A')}\")\n",
    "print(f\"Loss final: {train_result_ta_pt.metrics.get('train_loss', 'N/A'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a735249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ta_pt.save_pretrained(os.path.join(CONFIG[\"models_dir\"], \"ta_to_pt_final\"))\n",
    "tokenizer.save_pretrained(os.path.join(CONFIG[\"models_dir\"], \"ta_to_pt_final\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7ef53e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Avaliação Fine-Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b45418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "def load_finetuned_model(model_path):\n",
    "    \"\"\"Carrega um modelo fine-tuned com LoRA.\"\"\"\n",
    "    base_model = MBartForConditionalGeneration.from_pretrained(\n",
    "        CONFIG[\"model_checkpoint\"],\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model_pt_ta_ft = load_finetuned_model(os.path.join(CONFIG[\"models_dir\"], \"pt_to_ta_final\"))\n",
    "model_ta_pt_ft = load_finetuned_model(os.path.join(CONFIG[\"models_dir\"], \"ta_to_pt_final\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ca6018",
   "metadata": {},
   "source": [
    "### 7.1 Avaliação PT → TA (Fine-Tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca39e3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Traduzindo PT → TA (fine-tuned)...\")\n",
    "translations_pt_ta_ft = translate_batch(\n",
    "    model_pt_ta_ft,\n",
    "    test_portuguese,\n",
    "    source_lang=LANG_CODE,\n",
    "    target_lang=LANG_CODE\n",
    ")\n",
    "\n",
    "metrics_pt_ta_ft = compute_all_metrics(translations_pt_ta_ft, test_tupi_ref)\n",
    "\n",
    "print(\"\\n=== Métricas Fine-Tuned PT → TA ===\")\n",
    "print(f\"  BLEU:  {metrics_pt_ta_ft['bleu']:.2f}\")\n",
    "print(f\"  chrF1: {metrics_pt_ta_ft['chrf1']:.2f}\")\n",
    "print(f\"  chrF3: {metrics_pt_ta_ft['chrf3']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beb750a",
   "metadata": {},
   "source": [
    "### 7.2 Avaliação TA → PT (Fine-Tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2b7bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Traduzindo TA → PT (fine-tuned)...\")\n",
    "translations_ta_pt_ft = translate_batch(\n",
    "    model_ta_pt_ft,\n",
    "    test_tupi,\n",
    "    source_lang=LANG_CODE,\n",
    "    target_lang=LANG_CODE\n",
    ")\n",
    "\n",
    "metrics_ta_pt_ft = compute_all_metrics(translations_ta_pt_ft, test_portuguese_ref)\n",
    "\n",
    "print(\"\\n=== Métricas Fine-Tuned TA → PT ===\")\n",
    "print(f\"  BLEU:  {metrics_ta_pt_ft['bleu']:.2f}\")\n",
    "print(f\"  chrF1: {metrics_ta_pt_ft['chrf1']:.2f}\")\n",
    "print(f\"  chrF3: {metrics_ta_pt_ft['chrf3']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac903b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_few_shot = {\n",
    "    \"pt_to_ta\": {\n",
    "        \"bleu\": metrics_pt_ta_ft[\"bleu\"],\n",
    "        \"chrf1\": metrics_pt_ta_ft[\"chrf1\"],\n",
    "        \"chrf3\": metrics_pt_ta_ft[\"chrf3\"],\n",
    "        \"bleu_details\": metrics_pt_ta_ft[\"bleu_details\"]\n",
    "    },\n",
    "    \"ta_to_pt\": {\n",
    "        \"bleu\": metrics_ta_pt_ft[\"bleu\"],\n",
    "        \"chrf1\": metrics_ta_pt_ft[\"chrf1\"],\n",
    "        \"chrf3\": metrics_ta_pt_ft[\"chrf3\"],\n",
    "        \"bleu_details\": metrics_ta_pt_ft[\"bleu_details\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(CONFIG[\"results_dir\"], \"results_few_shot.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_few_shot, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "os.makedirs(os.path.join(CONFIG[\"results_dir\"], \"outputs_few_shot\"), exist_ok=True)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"source\": test_portuguese,\n",
    "    \"reference\": test_tupi_ref,\n",
    "    \"translation\": translations_pt_ta_ft\n",
    "}).to_csv(os.path.join(CONFIG[\"results_dir\"], \"outputs_few_shot\", \"pt_to_ta.csv\"), index=False)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"source\": test_tupi,\n",
    "    \"reference\": test_portuguese_ref,\n",
    "    \"translation\": translations_ta_pt_ft\n",
    "}).to_csv(os.path.join(CONFIG[\"results_dir\"], \"outputs_few_shot\", \"ta_to_pt.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec88ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Comparação: Zero-Shot vs Fine-Tuned\n",
    "\n",
    "### Matriz Comparativa de Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc786c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_data = {\n",
    "    \"Direção\": [\"PT → TA\", \"PT → TA\", \"TA → PT\", \"TA → PT\"],\n",
    "    \"Modo\": [\"Zero-Shot\", \"Fine-Tuned\", \"Zero-Shot\", \"Fine-Tuned\"],\n",
    "    \"BLEU\": [\n",
    "        metrics_pt_ta_zero[\"bleu\"],\n",
    "        metrics_pt_ta_ft[\"bleu\"],\n",
    "        metrics_ta_pt_zero[\"bleu\"],\n",
    "        metrics_ta_pt_ft[\"bleu\"]\n",
    "    ],\n",
    "    \"chrF1\": [\n",
    "        metrics_pt_ta_zero[\"chrf1\"],\n",
    "        metrics_pt_ta_ft[\"chrf1\"],\n",
    "        metrics_ta_pt_zero[\"chrf1\"],\n",
    "        metrics_ta_pt_ft[\"chrf1\"]\n",
    "    ],\n",
    "    \"chrF3\": [\n",
    "        metrics_pt_ta_zero[\"chrf3\"],\n",
    "        metrics_pt_ta_ft[\"chrf3\"],\n",
    "        metrics_ta_pt_zero[\"chrf3\"],\n",
    "        metrics_ta_pt_ft[\"chrf3\"]\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"=== Matriz Comparativa de Métricas ===\\n\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Calcular melhorias\n",
    "print(\"\\n\\n=== Melhoria com Fine-Tuning ===\")\n",
    "print(f\"\\nPT → TA:\")\n",
    "print(f\"  BLEU:  {metrics_pt_ta_ft['bleu'] - metrics_pt_ta_zero['bleu']:+.2f} pontos\")\n",
    "print(f\"  chrF1: {metrics_pt_ta_ft['chrf1'] - metrics_pt_ta_zero['chrf1']:+.2f} pontos\")\n",
    "print(f\"  chrF3: {metrics_pt_ta_ft['chrf3'] - metrics_pt_ta_zero['chrf3']:+.2f} pontos\")\n",
    "\n",
    "print(f\"\\nTA → PT:\")\n",
    "print(f\"  BLEU:  {metrics_ta_pt_ft['bleu'] - metrics_ta_pt_zero['bleu']:+.2f} pontos\")\n",
    "print(f\"  chrF1: {metrics_ta_pt_ft['chrf1'] - metrics_ta_pt_zero['chrf1']:+.2f} pontos\")\n",
    "print(f\"  chrF3: {metrics_ta_pt_ft['chrf3'] - metrics_ta_pt_zero['chrf3']:+.2f} pontos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d535f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    metrics_names = [\"BLEU\", \"chrF1\", \"chrF3\"]\n",
    "    x = np.arange(2)\n",
    "    width = 0.35\n",
    "    \n",
    "    for idx, metric in enumerate(metrics_names):\n",
    "        ax = axes[idx]\n",
    "        zero_shot = [metrics_pt_ta_zero[metric.lower()], metrics_ta_pt_zero[metric.lower()]]\n",
    "        fine_tuned = [metrics_pt_ta_ft[metric.lower()], metrics_ta_pt_ft[metric.lower()]]\n",
    "        \n",
    "        bars1 = ax.bar(x - width/2, zero_shot, width, label='Zero-Shot', color='#ff7f0e')\n",
    "        bars2 = ax.bar(x + width/2, fine_tuned, width, label='Fine-Tuned', color='#1f77b4')\n",
    "        \n",
    "        ax.set_xlabel('Direção')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.set_title(f'{metric}')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(['PT → TA', 'TA → PT'])\n",
    "        ax.legend()\n",
    "        ax.set_ylim(0, 100)\n",
    "        \n",
    "        for bar in bars1 + bars2:\n",
    "            height = bar.get_height()\n",
    "            ax.annotate(f'{height:.1f}',\n",
    "                       xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                       xytext=(0, 3),\n",
    "                       textcoords=\"offset points\",\n",
    "                       ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.suptitle('Comparação: Zero-Shot vs Fine-Tuned', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG[\"results_dir\"], \"comparison_chart.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"\\nGráfico salvo em {CONFIG['results_dir']}/comparison_chart.png\")\n",
    "except ImportError:\n",
    "    print(\"matplotlib não disponível para visualização gráfica\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
