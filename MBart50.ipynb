{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08b684aa-f5b1-4465-b4a2-44208ee6a05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    MBart50TokenizerFast,\n",
    "    MBartForConditionalGeneration,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "648f598b-7200-4563-9415-2f9202e81760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuração ---\n",
    "MODEL_CHECKPOINT = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "SOURCE_LANG = \"en_XX\" # Inglês\n",
    "TARGET_LANG = \"pt_XX\" # Português\n",
    "MAX_INPUT_LENGTH = 128\n",
    "MAX_TARGET_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5795d02f-a977-42ef-aa8b-1e34ef901845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 1404\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Um córpus em paralelo com diversas línguas\n",
    "dataset = load_dataset(\"opus_books\", \"en-pt\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be863ab2-017c-4c70-b2bc-db4e5398fae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use o train_test_split para dividir em treino e teste\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a16a40-d1ee-47cb-bd20-60fff058c14e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 1123\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 281\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46c53f50-fc48-436a-b9ab-8be6771da5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'translation'])\n",
      "dict_keys(['en', 'pt'])\n",
      "\ten: Alice crouched down among the trees as well as she could, for her neck kept getting entangled among the branches, and every now and then she had to stop and untwist it.\n",
      "\tpt: Alice agachou-se entre as árvores o melhor que pode, pois o pescoço dela continuou se enredando entre os galhos, e aqui e ali ela tinha que parar e desenredá-lo.\n"
     ]
    }
   ],
   "source": [
    "# Espiadinha nos dados\n",
    "print(dataset[\"train\"][0].keys())\n",
    "\n",
    "print(dataset[\"train\"][0][\"translation\"].keys())\n",
    "\n",
    "print(\"\\ten:\", dataset[\"train\"][0][\"translation\"][\"en\"])\n",
    "print(\"\\tpt:\", dataset[\"train\"][0][\"translation\"][\"pt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "230acef8-0b13-4cde-8e3a-e669e664fcbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f5c04aa68f4623bae1ecf09be23925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf47207e77943aa97df9bc443f5bb97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = MBart50TokenizerFast.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"pt\"] for ex in examples[\"translation\"]]\n",
    "\n",
    "    # 1. Definir a língua fonte (Essencial para o mBART)\n",
    "    tokenizer.src_lang = SOURCE_LANG\n",
    "    \n",
    "    # 2. Definir a língua alvo (Essencial para o mBART)\n",
    "    tokenizer.tgt_lang = TARGET_LANG\n",
    "\n",
    "    # 3. Tokeniza o texto de entrada (texto em inglês)\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n",
    "\n",
    "    # 4. Tokenize o texto alvo (portugês)\n",
    "    labels = tokenizer(\n",
    "        text_target=targets, \n",
    "        max_length=MAX_TARGET_LENGTH, \n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Mapeia a função de tokenização no dataset\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320873f7-198d-4fde-ab0e-c4310d7f96a2",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b376b9ff-90b5-4699-b665-59327ad4a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Carrega o modelo base\n",
    "model = MBartForConditionalGeneration.from_pretrained(\n",
    "    MODEL_CHECKPOINT,\n",
    "    device_map=\"auto\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2326e80c-c1b2-4f55-909c-9d52e7af6a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MBartForConditionalGeneration(\n",
       "  (model): MBartModel(\n",
       "    (shared): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "    (encoder): MBartEncoder(\n",
       "      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x MBartEncoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): MBartDecoder(\n",
       "      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x MBartDecoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250054, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "049a0cb8-5600-479d-8bd9-8488caef8cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(model.base_model.model.forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efe8bd5-10b1-49b9-9842-a72f934681f5",
   "metadata": {},
   "source": [
    "# Opcional utilizar peft para realizar LoRa\n",
    "Utilizando a biblioteca [PEFT](https://huggingface.co/docs/peft/index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af137446-288b-44d9-aa61-94fa4f6f83d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,359,296 || all params: 613,238,784 || trainable%: 0.3847\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "# 2. Define a configuração do LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16, \n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], # Aplica nas camadas de atenção\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "# 3.Envelopa o modelo com PEFT\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "# Os parâmetros treináveis devem ser aproximadamente 0.5% ou menos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e370b832-1467-48bf-b268-41339928e6d2",
   "metadata": {},
   "source": [
    "# Configuração de avaliação do modelo\n",
    "- Utilizando somente [BLEU](https://huggingface.co/spaces/evaluate-metric/bleu)\n",
    "- Diferença entre sacrebleu e bleu [A Call for Clarity in Reporting BLEU Scores](https://arxiv.org/abs/1804.08771)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9278b4ce-4ef4-479f-9512-15d618f4e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    \n",
    "    # Para caso o modelo returne mais do que os logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Substitui -100 nos rótulos já que não dá para decodificá-los\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # BLEU recebe uma lista de listas para referências\n",
    "    decoded_labels = [[l] for l in decoded_labels]\n",
    "\n",
    "    # Aqui vocês podem adicionar outras métricas\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63c3710a-d5e9-4eb1-b5b9-d56d67f3e4a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='281' max='281' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [281/281 00:43, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.606027</td>\n",
       "      <td>32.593299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model) \n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./mbart50-finetuned\",\n",
    "    learning_rate=1e-4, # 5e-5 Consultar a documentação / artigo do modelo\n",
    "    per_device_train_batch_size=4, # Ideal é aumentar até chegar no limite da memória\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    predict_with_generate=True, \n",
    "    bf16=True, # Treina com precisão mais baixa, porém mais rápida\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "train_output = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e34eb8ac-e1d7-4242-9b24-8618ecc594a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=281, training_loss=1.6249434175864657, metrics={'train_runtime': 43.8167, 'train_samples_per_second': 25.63, 'train_steps_per_second': 6.413, 'total_flos': 125986729574400.0, 'train_loss': 1.6249434175864657, 'epoch': 1.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f606b85-7729-4467-8cea-348e850204fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8753130558531335\n"
     ]
    }
   ],
   "source": [
    "# Número de teraflops utilizados\n",
    "# Número total operaçõeS de número de ponto-flutuante (total_flos)\n",
    "print(train_output.metrics[\"total_flos\"] / (train_output.metrics[\"train_runtime\"] *  1e12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b75976-aac4-4bf7-8982-f9bb581cc191",
   "metadata": {},
   "source": [
    "# Gera Tradução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84510594-1dd0-4843-8369-b9758a78e84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3c238f94d743ef84a2608490bee302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='The students are learning about natural language processing today.', layout=Lay…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "input_text = widgets.Textarea(\n",
    "    value=\"The students are learning about natural language processing today.\",\n",
    "    placeholder=\"Type English text here...\",\n",
    "    layout={'width': '100%'}\n",
    ")\n",
    "button = widgets.Button(description=\"Translate\", button_style='primary')\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_click(b):\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        # Garante que a língua fonte está definida\n",
    "        tokenizer.src_lang = SOURCE_LANG \n",
    "        \n",
    "        # Tokeniza e move para o mesmo dispositivo onde o modelo está model (GPU/CPU)\n",
    "        inputs = tokenizer(input_text.value, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        # Gera o texto forçando a língua alvo\n",
    "        generated_tokens = model.generate(\n",
    "            **inputs, \n",
    "            forced_bos_token_id=tokenizer.lang_code_to_id[TARGET_LANG]\n",
    "        )\n",
    "        \n",
    "        # Decodifica e imprime\n",
    "        result = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "        print(f\"Prediction: {result}\")\n",
    "\n",
    "button.on_click(on_click)\n",
    "display(widgets.VBox([input_text, button, output]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
