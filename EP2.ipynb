{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a890932b",
   "metadata": {},
   "source": [
    "cuda\n",
    "cpu\n",
    "Dispositivo: {device}\n",
    "PyTorch version: {torch.__version__}\n",
    "\n",
    "### Instalação de Dependências\n",
    "Primeiro, garantimos que todas as bibliotecas necessárias estejam instaladas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4c029e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo selecionado: cpu (tentando usar cuda)\n",
      "PyTorch version: 2.9.1+cpu\n",
      "GPU não detectada no momento.\n",
      "  → Se você tem uma GPU NVIDIA, confirme que os drivers/cuda estão instalados (execute `nvidia-smi`).\n",
      "  → Instale a versão CUDA do PyTorch: `pip install 'torch==2.9.1+cu118' --index-url https://download.pytorch.org/whl/cu118`.\n",
      "  → Caso haja múltiplas GPUs, defina `CUDA_VISIBLE_DEVICES` antes de iniciar o notebook (por exemplo `CUDA_VISIBLE_DEVICES=0 jupyter lab`).\n"
     ]
    }
   ],
   "source": [
    "# Instalação das dependências (executar apenas uma vez)\n",
    "# !pip install transformers datasets evaluate sacrebleu pandas openpyxl torch peft sentencepiece\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Imports principais\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Transformers e Datasets\n",
    "from transformers import (\n",
    "    MBart50TokenizerFast,\n",
    "    MBartForConditionalGeneration,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Avaliação\n",
    "import evaluate\n",
    "\n",
    "# PEFT para LoRA (opcional)\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Configuração do dispositivo\n",
    "preferred_device = \"cuda\"\n",
    "device = torch.device(preferred_device if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo selecionado: {device} (tentando usar {preferred_device})\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memória GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"GPU não detectada no momento.\")\n",
    "    print(\"  → Se você tem uma GPU NVIDIA, confirme que os drivers/cuda estão instalados (execute `nvidia-smi`).\")\n",
    "    print(\"  → Instale a versão CUDA do PyTorch: `pip install 'torch==2.9.1+cu118' --index-url https://download.pytorch.org/whl/cu118`.\")\n",
    "    print(\"  → Caso haja múltiplas GPUs, defina `CUDA_VISIBLE_DEVICES` antes de iniciar o notebook (por exemplo `CUDA_VISIBLE_DEVICES=0 jupyter lab`).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30f6e2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurações carregadas:\n",
      "  model_checkpoint: facebook/mbart-large-50-many-to-many-mmt\n",
      "  max_input_length: 128\n",
      "  max_target_length: 128\n",
      "  learning_rate: 5e-05\n",
      "  batch_size: 4\n",
      "  num_epochs: 10\n",
      "  weight_decay: 0.01\n",
      "  warmup_steps: 100\n",
      "  early_stopping_patience: 3\n",
      "  lora_r: 16\n",
      "  lora_alpha: 32\n",
      "  lora_dropout: 0.05\n",
      "  data_path: ./data.xlsx\n",
      "  results_dir: ./results\n",
      "  models_dir: ./models\n",
      "  seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Configurações globais do projeto (otimizadas para LoRA + corpus pequeno)\n",
    "CONFIG = {\n",
    "    # Modelo\n",
    "    \"model_checkpoint\": \"facebook/mbart-large-50-many-to-many-mmt\",\n",
    "    \n",
    "    # Tokenização\n",
    "    \"max_input_length\": 128,\n",
    "    \"max_target_length\": 128,\n",
    "    \n",
    "    # Treinamento (otimizado para LoRA com mBART)\n",
    "    \"learning_rate\": 3e-5,  # Reduzido: melhor para LoRA\n",
    "    \"batch_size\": 4,\n",
    "    \"num_epochs\": 15,  # Aumentado: LoRA converge mais devagar\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_ratio\": 0.1,  # Usar ratio ao invés de steps fixos\n",
    "    \n",
    "    # Early stopping\n",
    "    \"early_stopping_patience\": 5,  # Aumentado: dar mais tempo para convergir\n",
    "    \n",
    "    # LoRA (Low-Rank Adaptation) - otimizado\n",
    "    \"lora_r\": 32,  # Aumentado: mais capacidade para aprender Tupi\n",
    "    \"lora_alpha\": 64,  # Mantém proporção alpha = 2*r\n",
    "    \"lora_dropout\": 0.1,  # Aumentado: mais regularização para corpus pequeno\n",
    "    \n",
    "    # Caminhos\n",
    "    \"data_path\": \"./data.xlsx\",\n",
    "    \"results_dir\": \"./results\",\n",
    "    \"models_dir\": \"./models\",\n",
    "    \n",
    "    # Seed para reprodutibilidade\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "# Criar diretórios\n",
    "os.makedirs(CONFIG[\"results_dir\"], exist_ok=True)\n",
    "os.makedirs(CONFIG[\"models_dir\"], exist_ok=True)\n",
    "\n",
    "print(\"Configurações carregadas (otimizadas para LoRA):\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d9f375",
   "metadata": {},
   "source": [
    "## 2. Justificativa do Modelo Escolhido\n",
    "\n",
    "### Por que mBART-50?\n",
    "\n",
    "Escolhemos o modelo **facebook/mbart-large-50-many-to-many-mmt** pelos seguintes motivos:\n",
    "\n",
    "1. **Arquitetura Encoder-Decoder**: O mBART utiliza a arquitetura Transformer completa (encoder-decoder), ideal para tarefas de tradução automática, diferente de modelos apenas decoder (GPT) ou apenas encoder (BERT).\n",
    "\n",
    "2. **Pré-treinamento Multilíngue**: O modelo foi pré-treinado em 50 idiomas, incluindo o Português. Embora não inclua Tupi Antigo, o conhecimento multilíngue pode ajudar na transferência de padrões linguísticos.\n",
    "\n",
    "3. **Many-to-Many**: Esta variante permite tradução entre qualquer par de idiomas suportados, facilitando a adaptação para novos pares linguísticos.\n",
    "\n",
    "4. **Suporte a Línguas de Baixo Recurso**: O mBART foi projetado especificamente para cenários de baixo recurso, onde há poucos dados de treinamento disponíveis.\n",
    "\n",
    "5. **Fine-tuning Eficiente**: Com técnicas como LoRA (Low-Rank Adaptation), podemos fazer fine-tuning eficiente mesmo com recursos computacionais limitados.\n",
    "\n",
    "### Alternativas Consideradas\n",
    "\n",
    "| Modelo | Prós | Contras |\n",
    "|--------|------|---------|\n",
    "| **mBART-50** | Multilíngue, encoder-decoder, bom para baixo recurso | Grande (1.2GB), requer GPU |\n",
    "| **NLLB-200** | 200 idiomas, otimizado para tradução | Muito grande, pode ser lento |\n",
    "| **mT5** | Flexível, multilíngue | Não específico para tradução |\n",
    "| **T5** | Leve, rápido | Focado em inglês |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab89da7c",
   "metadata": {},
   "source": [
    "## 3. Explicação Matemática das Métricas\n",
    "\n",
    "### 3.1 BLEU (Bilingual Evaluation Understudy)\n",
    "\n",
    "O BLEU mede a similaridade entre uma tradução candidata e uma ou mais traduções de referência usando n-gramas.\n",
    "\n",
    "**Fórmula:**\n",
    "\n",
    "$$\\text{BLEU} = BP \\cdot \\exp\\left(\\sum_{n=1}^{N} w_n \\log p_n\\right)$$\n",
    "\n",
    "Onde:\n",
    "- $p_n$ é a precisão do n-grama: $p_n = \\frac{\\text{n-gramas correspondentes}}{\\text{total de n-gramas na candidata}}$\n",
    "- $w_n$ é o peso para cada n-grama (geralmente $w_n = 1/N$)\n",
    "- $BP$ é a penalidade de brevidade:\n",
    "\n",
    "$$BP = \\begin{cases} 1 & \\text{se } c > r \\\\ e^{1-r/c} & \\text{se } c \\leq r \\end{cases}$$\n",
    "\n",
    "Onde $c$ é o comprimento da candidata e $r$ é o comprimento da referência.\n",
    "\n",
    "### 3.2 chrF (Character-level F-score)\n",
    "\n",
    "O chrF utiliza n-gramas de **caracteres** ao invés de palavras, sendo mais robusto para línguas morfologicamente ricas.\n",
    "\n",
    "**Fórmula:**\n",
    "\n",
    "$$\\text{chrF}_\\beta = (1 + \\beta^2) \\cdot \\frac{\\text{chrP} \\cdot \\text{chrR}}{\\beta^2 \\cdot \\text{chrP} + \\text{chrR}}$$\n",
    "\n",
    "Onde:\n",
    "- $\\text{chrP}$ = Precisão de n-gramas de caracteres\n",
    "- $\\text{chrR}$ = Recall de n-gramas de caracteres\n",
    "- $\\beta$ = Peso do recall (chrF1: $\\beta=1$, chrF3: $\\beta=3$)\n",
    "\n",
    "**chrF1** ($\\beta=1$): Dá peso igual para precisão e recall.\n",
    "\n",
    "**chrF3** ($\\beta=3$): Dá mais peso ao recall, útil quando queremos capturar mais do conteúdo da referência.\n",
    "\n",
    "### Por que chrF é importante para Tupi Antigo?\n",
    "\n",
    "O Tupi Antigo possui morfologia complexa com prefixos, sufixos e variações ortográficas históricas. Métricas baseadas em caracteres capturam melhor essas nuances do que métricas baseadas em palavras.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a1f64a",
   "metadata": {},
   "source": [
    "## 4. Carregamento e Preparação dos Dados\n",
    "\n",
    "### 4.1 Leitura do Corpus\n",
    "\n",
    "O corpus paralelo Português ↔ Tupi Antigo está armazenado em `data.xlsx` com as colunas:\n",
    "- **Português**: Frases em português\n",
    "- **Tupi Antigo**: Traduções correspondentes em Tupi Antigo\n",
    "\n",
    "**Importante**: Preservamos acentos, diacríticos e grafia histórica do Tupi Antigo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "031e902d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus carregado: 7097 pares de frases\n",
      "\n",
      "Colunas: ['Português', 'Tupi Antigo']\n",
      "\n",
      "Primeiras 5 linhas:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Português</th>\n",
       "      <th>Tupi Antigo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aparei as pontas deles</td>\n",
       "      <td>Aîapyr-etab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Doravante assim procedo</td>\n",
       "      <td>Ko'yré emonã aîkó</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As doenças da alma do homem com ele saram bem</td>\n",
       "      <td>Abá 'anga mara'ara i pupé opûeîrá-katu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>É discreta, falando aos homens</td>\n",
       "      <td>I kunusãî abá supé onhe'enga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Para se vingar de seu cão que cria, um homem n...</td>\n",
       "      <td>O eîmbaba îagûara resé oîepyka, abá n'oîmomba'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Português  \\\n",
       "0                             Aparei as pontas deles   \n",
       "1                            Doravante assim procedo   \n",
       "2      As doenças da alma do homem com ele saram bem   \n",
       "3                     É discreta, falando aos homens   \n",
       "4  Para se vingar de seu cão que cria, um homem n...   \n",
       "\n",
       "                                         Tupi Antigo  \n",
       "0                                        Aîapyr-etab  \n",
       "1                                  Ko'yré emonã aîkó  \n",
       "2             Abá 'anga mara'ara i pupé opûeîrá-katu  \n",
       "3                       I kunusãî abá supé onhe'enga  \n",
       "4  O eîmbaba îagûara resé oîepyka, abá n'oîmomba'...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leitura do corpus\n",
    "df = pd.read_excel(CONFIG[\"data_path\"])\n",
    "\n",
    "# Normalizar nomes das colunas (corrige encoding PortuguÊs -> Português)\n",
    "df.columns = df.columns.str.replace('PortuguÊs', 'Português', regex=False)\n",
    "\n",
    "print(f\"Corpus carregado: {len(df)} pares de frases\")\n",
    "print(f\"\\nColunas: {list(df.columns)}\")\n",
    "print(f\"\\nPrimeiras 5 linhas:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f8a8a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informações do DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7097 entries, 0 to 7096\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Português    7097 non-null   object\n",
      " 1   Tupi Antigo  7097 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 111.0+ KB\n",
      "None\n",
      "\n",
      "Valores nulos:\n",
      "Português      0\n",
      "Tupi Antigo    0\n",
      "dtype: int64\n",
      "\n",
      "Estatísticas de comprimento (caracteres):\n",
      "            len_pt       len_ta\n",
      "count  7097.000000  7097.000000\n",
      "mean     35.728054    26.222911\n",
      "std      21.140072    17.144765\n",
      "min       3.000000     2.000000\n",
      "25%      20.000000    13.000000\n",
      "50%      31.000000    21.000000\n",
      "75%      47.000000    36.000000\n",
      "max     187.000000   136.000000\n"
     ]
    }
   ],
   "source": [
    "# Verificar informações do dataset\n",
    "print(\"Informações do DataFrame:\")\n",
    "print(df.info())\n",
    "print(f\"\\nValores nulos:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nEstatísticas de comprimento (caracteres):\")\n",
    "df['len_pt'] = df['Português'].astype(str).str.len()\n",
    "df['len_ta'] = df['Tupi Antigo'].astype(str).str.len()\n",
    "print(df[['len_pt', 'len_ta']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af7ec89",
   "metadata": {},
   "source": [
    "### 4.2 Limpeza e Normalização Mínima\n",
    "\n",
    "Realizamos apenas limpeza mínima para preservar as características linguísticas do Tupi Antigo:\n",
    "\n",
    "1. **Remoção de espaços extras** (início, fim, múltiplos espaços)\n",
    "2. **Remoção de caracteres invisíveis** (zero-width spaces, etc.)\n",
    "3. **NÃO removemos**: acentos, diacríticos, maiúsculas/minúsculas\n",
    "\n",
    "**Justificativa**: O Tupi Antigo possui variações ortográficas históricas e símbolos especiais que carregam significado linguístico. Uma normalização agressiva poderia remover informações importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "453d6f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Após limpeza: 7097 pares de frases\n",
      "\n",
      "Exemplos após limpeza:\n",
      "\n",
      "[1] PT: Aparei as pontas deles\n",
      "    TA: Aîapyr-etab\n",
      "\n",
      "[2] PT: Doravante assim procedo\n",
      "    TA: Ko'yré emonã aîkó\n",
      "\n",
      "[3] PT: As doenças da alma do homem com ele saram bem\n",
      "    TA: Abá 'anga mara'ara i pupé opûeîrá-katu\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Limpeza mínima do texto, preservando acentos e diacríticos.\n",
    "    \n",
    "    Args:\n",
    "        text: Texto a ser limpo\n",
    "        \n",
    "    Returns:\n",
    "        Texto limpo\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Remover caracteres invisíveis (zero-width spaces, etc.)\n",
    "    text = re.sub(r'[\\u200b\\u200c\\u200d\\ufeff]', '', text)\n",
    "    \n",
    "    # Remover espaços extras (múltiplos espaços -> um espaço)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remover espaços no início e fim\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Aplicar limpeza\n",
    "df['Português'] = df['Português'].apply(clean_text)\n",
    "df['Tupi Antigo'] = df['Tupi Antigo'].apply(clean_text)\n",
    "\n",
    "# Remover linhas vazias\n",
    "df = df[(df['Português'] != '') & (df['Tupi Antigo'] != '')]\n",
    "\n",
    "# Remover colunas auxiliares de comprimento\n",
    "df = df.drop(columns=['len_pt', 'len_ta'], errors='ignore')\n",
    "\n",
    "print(f\"Após limpeza: {len(df)} pares de frases\")\n",
    "print(\"\\nExemplos após limpeza:\")\n",
    "for i in range(min(3, len(df))):\n",
    "    print(f\"\\n[{i+1}] PT: {df.iloc[i]['Português']}\")\n",
    "    print(f\"    TA: {df.iloc[i]['Tupi Antigo']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef89123c",
   "metadata": {},
   "source": [
    "### 4.3 Divisão do Corpus\n",
    "\n",
    "Dividimos o corpus em três conjuntos:\n",
    "- **Treino (70%)**: Para fine-tuning do modelo\n",
    "- **Validação (15%)**: Para early stopping e seleção de hiperparâmetros\n",
    "- **Teste (15%)**: Para avaliação final (nunca usado durante treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89c8f07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divisão do corpus:\n",
      "  Treino:     4967 (70.0%)\n",
      "  Validação:  1065 (15.0%)\n",
      "  Teste:      1065 (15.0%)\n",
      "  Total:      7097\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Definir seed para reprodutibilidade\n",
    "np.random.seed(CONFIG[\"seed\"])\n",
    "\n",
    "# Primeira divisão: 70% treino, 30% (validação + teste)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.30, random_state=CONFIG[\"seed\"])\n",
    "\n",
    "# Segunda divisão: 50% validação, 50% teste (do temp_df)\n",
    "# Isso resulta em 15% validação e 15% teste do total\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.50, random_state=CONFIG[\"seed\"])\n",
    "\n",
    "print(f\"Divisão do corpus:\")\n",
    "print(f\"  Treino:     {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Validação:  {len(val_df)} ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Teste:      {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Total:      {len(df)}\")\n",
    "\n",
    "# Reset dos índices\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb83f44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subconjuntos salvos em ./data/\n",
      "  train.csv: 4967 exemplos\n",
      "  val.csv:   1065 exemplos\n",
      "  test.csv:  1065 exemplos\n"
     ]
    }
   ],
   "source": [
    "# Salvar os subconjuntos para uso posterior\n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "\n",
    "train_df.to_csv(\"./data/train.csv\", index=False)\n",
    "val_df.to_csv(\"./data/val.csv\", index=False)\n",
    "test_df.to_csv(\"./data/test.csv\", index=False)\n",
    "\n",
    "print(\"Subconjuntos salvos em ./data/\")\n",
    "print(f\"  train.csv: {len(train_df)} exemplos\")\n",
    "print(f\"  val.csv:   {len(val_df)} exemplos\")\n",
    "print(f\"  test.csv:  {len(test_df)} exemplos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97f54234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Hugging Face criado:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['portuguese', 'tupi'],\n",
      "        num_rows: 4967\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['portuguese', 'tupi'],\n",
      "        num_rows: 1065\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['portuguese', 'tupi'],\n",
      "        num_rows: 1065\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Converter para Datasets do Hugging Face\n",
    "def df_to_dataset(df):\n",
    "    \"\"\"Converte DataFrame para Dataset do Hugging Face.\"\"\"\n",
    "    return Dataset.from_dict({\n",
    "        \"portuguese\": df[\"Português\"].tolist(),\n",
    "        \"tupi\": df[\"Tupi Antigo\"].tolist()\n",
    "    })\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": df_to_dataset(train_df),\n",
    "    \"validation\": df_to_dataset(val_df),\n",
    "    \"test\": df_to_dataset(test_df)\n",
    "})\n",
    "\n",
    "print(\"Dataset Hugging Face criado:\")\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3140e7a",
   "metadata": {},
   "source": [
    "## 5. Configuração do Modelo e Tokenizador\n",
    "\n",
    "### Tratamento do Tupi Antigo no mBART\n",
    "\n",
    "O mBART não possui código de idioma nativo para Tupi Antigo. Utilizamos uma estratégia de adaptação:\n",
    "\n",
    "1. Usamos o código de idioma do Português (`pt_XX`) como proxy para ambos os idiomas\n",
    "2. Adicionamos prefixos textuais nas entradas para indicar a direção da tradução\n",
    "3. O modelo aprende a associar esses padrões durante o fine-tuning\n",
    "\n",
    "**Nota**: Esta é uma limitação do cenário de baixo recurso. Em um cenário ideal, teríamos um tokenizador e código de idioma específico para Tupi Antigo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a918f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da84e6cb07634eb884e4ada704e332cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da84e6cb07634eb884e4ada704e332cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e220005b084e58947bb963d27a0c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da84e6cb07634eb884e4ada704e332cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e220005b084e58947bb963d27a0c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b00e101abd46fbada0be7e18b37167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da84e6cb07634eb884e4ada704e332cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e220005b084e58947bb963d27a0c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b00e101abd46fbada0be7e18b37167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca064c166864ba49b9874740ccb91d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da84e6cb07634eb884e4ada704e332cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e220005b084e58947bb963d27a0c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b00e101abd46fbada0be7e18b37167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca064c166864ba49b9874740ccb91d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizador carregado: facebook/mbart-large-50-many-to-many-mmt\n",
      "Vocabulário: 250054 tokens\n",
      "Código de idioma usado: pt_XX\n"
     ]
    }
   ],
   "source": [
    "# Carregar tokenizador\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(CONFIG[\"model_checkpoint\"])\n",
    "\n",
    "# Configuração de idiomas\n",
    "# Usamos pt_XX como proxy para ambos (PT e Tupi Antigo)\n",
    "# O modelo aprenderá a distinção através dos prefixos e do fine-tuning\n",
    "LANG_CODE = \"pt_XX\"\n",
    "\n",
    "# Prefixos para indicar direção da tradução\n",
    "# Isso ajuda o modelo a distinguir as tarefas durante o fine-tuning\n",
    "PREFIX_PT_TO_TA = \"traduzir para tupi: \"\n",
    "PREFIX_TA_TO_PT = \"traduzir para português: \"\n",
    "\n",
    "print(f\"Tokenizador carregado: {CONFIG['model_checkpoint']}\")\n",
    "print(f\"Vocabulário: {tokenizer.vocab_size} tokens\")\n",
    "print(f\"Código de idioma: {LANG_CODE}\")\n",
    "print(f\"Prefixo PT→TA: '{PREFIX_PT_TO_TA}'\")\n",
    "print(f\"Prefixo TA→PT: '{PREFIX_TA_TO_PT}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6528a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções de pré-processamento para cada direção de tradução\n",
    "\n",
    "def preprocess_pt_to_ta(examples):\n",
    "    \"\"\"Pré-processa para tradução Português -> Tupi Antigo.\"\"\"\n",
    "    inputs = examples[\"portuguese\"]\n",
    "    targets = examples[\"tupi\"]\n",
    "    \n",
    "    tokenizer.src_lang = LANG_CODE\n",
    "    tokenizer.tgt_lang = LANG_CODE\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=CONFIG[\"max_input_length\"], \n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    labels = tokenizer(\n",
    "        text_target=targets,\n",
    "        max_length=CONFIG[\"max_target_length\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_ta_to_pt(examples):\n",
    "    \"\"\"Pré-processa para tradução Tupi Antigo -> Português.\"\"\"\n",
    "    inputs = examples[\"tupi\"]\n",
    "    targets = examples[\"portuguese\"]\n",
    "    \n",
    "    tokenizer.src_lang = LANG_CODE\n",
    "    tokenizer.tgt_lang = LANG_CODE\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=CONFIG[\"max_input_length\"], \n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    labels = tokenizer(\n",
    "        text_target=targets,\n",
    "        max_length=CONFIG[\"max_target_length\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "print(\"Funções de pré-processamento definidas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953c99a9",
   "metadata": {},
   "source": [
    "## 6. Configuração das Métricas de Avaliação\n",
    "\n",
    "Implementamos as métricas conforme especificado no enunciado:\n",
    "- **BLEU**: Usando SacreBLEU para resultados reproduzíveis\n",
    "- **chrF1**: F-score de caracteres com β=1\n",
    "- **chrF3**: F-score de caracteres com β=3 (mais peso no recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66d21a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar métricas\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "chrf_metric = evaluate.load(\"chrf\")\n",
    "\n",
    "def compute_all_metrics(predictions, references):\n",
    "    \"\"\"\n",
    "    Calcula todas as métricas de avaliação.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Lista de traduções geradas\n",
    "        references: Lista de traduções de referência\n",
    "        \n",
    "    Returns:\n",
    "        Dicionário com todas as métricas\n",
    "    \"\"\"\n",
    "    # BLEU espera referências como lista de listas\n",
    "    refs_for_bleu = [[ref] for ref in references]\n",
    "    \n",
    "    # BLEU\n",
    "    bleu_result = bleu_metric.compute(predictions=predictions, references=refs_for_bleu)\n",
    "    \n",
    "    # chrF1 (beta=1)\n",
    "    chrf1_result = chrf_metric.compute(\n",
    "        predictions=predictions, \n",
    "        references=refs_for_bleu,\n",
    "        char_order=6,\n",
    "        word_order=0,\n",
    "        beta=1\n",
    "    )\n",
    "    \n",
    "    # chrF3 (beta=3)\n",
    "    chrf3_result = chrf_metric.compute(\n",
    "        predictions=predictions, \n",
    "        references=refs_for_bleu,\n",
    "        char_order=6,\n",
    "        word_order=0,\n",
    "        beta=3\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"bleu\": bleu_result[\"score\"],\n",
    "        \"chrf1\": chrf1_result[\"score\"],\n",
    "        \"chrf3\": chrf3_result[\"score\"],\n",
    "        \"bleu_details\": {\n",
    "            \"precisions\": bleu_result[\"precisions\"],\n",
    "            \"brevity_penalty\": bleu_result[\"bp\"],\n",
    "            \"length_ratio\": bleu_result[\"sys_len\"] / bleu_result[\"ref_len\"] if bleu_result[\"ref_len\"] > 0 else 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"Métricas carregadas e configuradas:\")\n",
    "print(\"  - BLEU (SacreBLEU)\")\n",
    "print(\"  - chrF1 (β=1)\")\n",
    "print(\"  - chrF3 (β=3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e82aa9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Tradução Zero-Shot\n",
    "\n",
    "No regime **zero-shot**, utilizamos o modelo pré-treinado diretamente, sem qualquer fine-tuning no corpus Português-Tupi Antigo.\n",
    "\n",
    "### Estratégia para Melhorar Zero-Shot\n",
    "\n",
    "Como o mBART-50 não foi treinado em Tupi Antigo, utilizamos estratégias de **transferência cross-lingual**:\n",
    "\n",
    "1. **Línguas Proxy**: Usamos línguas estruturalmente similares presentes no mBART:\n",
    "   - **Espanhol (es_XX)**: Proximidade lexical com Português e estrutura latina\n",
    "   - **Português (pt_XX)**: Língua base do corpus\n",
    "   \n",
    "2. **Múltiplas Tentativas**: Testamos diferentes configurações de idioma source/target para encontrar a melhor transferência\n",
    "\n",
    "3. **Otimização de Geração**: \n",
    "   - Beam search com mais beams\n",
    "   - Length penalty para controlar tamanho\n",
    "   - No repeat ngram para evitar repetições\n",
    "\n",
    "4. **Ensemble de Traduções**: Combinamos resultados de diferentes configurações\n",
    "\n",
    "**Hipótese**: Como Tupi Antigo possui elementos morfológicos complexos e aglutinação, línguas com características similares podem fornecer melhor transferência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fb78c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar modelo para zero-shot\n",
    "print(\"Carregando modelo mBART para zero-shot...\")\n",
    "model_zero_shot = MBartForConditionalGeneration.from_pretrained(\n",
    "    CONFIG[\"model_checkpoint\"],\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "model_zero_shot = model_zero_shot.to(device)\n",
    "model_zero_shot.eval()\n",
    "\n",
    "print(f\"Modelo carregado: {CONFIG['model_checkpoint']}\")\n",
    "print(f\"Parâmetros: {sum(p.numel() for p in model_zero_shot.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd27d6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de tradução otimizada para zero-shot\n",
    "def translate_batch_zeroshot(model, texts, source_lang, target_lang, batch_size=8, \n",
    "                              num_beams=10, length_penalty=1.0, no_repeat_ngram_size=3):\n",
    "    \"\"\"\n",
    "    Traduz um lote de textos usando o modelo com parâmetros otimizados para zero-shot.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo de tradução\n",
    "        texts: Lista de textos a traduzir\n",
    "        source_lang: Código do idioma fonte\n",
    "        target_lang: Código do idioma alvo\n",
    "        batch_size: Tamanho do lote\n",
    "        num_beams: Número de beams para beam search\n",
    "        length_penalty: Penalidade de comprimento (>1 = mais longo, <1 = mais curto)\n",
    "        no_repeat_ngram_size: Tamanho do n-gram para evitar repetições\n",
    "        \n",
    "    Returns:\n",
    "        Lista de traduções\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    translations = []\n",
    "    \n",
    "    tokenizer.src_lang = source_lang\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            batch, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True,\n",
    "            max_length=CONFIG[\"max_input_length\"]\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(\n",
    "                **inputs,\n",
    "                forced_bos_token_id=tokenizer.lang_code_to_id[target_lang],\n",
    "                max_length=CONFIG[\"max_target_length\"],\n",
    "                num_beams=num_beams,\n",
    "                length_penalty=length_penalty,\n",
    "                no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "                early_stopping=True,\n",
    "                do_sample=False  # Determinístico para reprodutibilidade\n",
    "            )\n",
    "        \n",
    "        decoded = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "        translations.extend(decoded)\n",
    "        \n",
    "        if (i // batch_size + 1) % 20 == 0:\n",
    "            print(f\"  Traduzidos: {min(i + batch_size, len(texts))}/{len(texts)}\")\n",
    "    \n",
    "    return translations\n",
    "\n",
    "# Configurações de línguas proxy para experimentação\n",
    "# mBART-50 suporta estas línguas que podem servir como proxy\n",
    "LANG_CONFIGS = {\n",
    "    \"pt_pt\": {\"source\": \"pt_XX\", \"target\": \"pt_XX\", \"name\": \"PT→PT (baseline)\"},\n",
    "    \"pt_es\": {\"source\": \"pt_XX\", \"target\": \"es_XX\", \"name\": \"PT→ES (proxy espanhol)\"},\n",
    "    \"es_pt\": {\"source\": \"es_XX\", \"target\": \"pt_XX\", \"name\": \"ES→PT (proxy espanhol)\"},\n",
    "    \"pt_gl\": {\"source\": \"pt_XX\", \"target\": \"gl_XX\", \"name\": \"PT→GL (proxy galego)\"},  # Galego não existe, mas podemos testar\n",
    "}\n",
    "\n",
    "# Línguas disponíveis no mBART-50 para referência\n",
    "print(\"Línguas disponíveis no mBART-50:\")\n",
    "available_langs = [k for k in tokenizer.lang_code_to_id.keys()]\n",
    "print(f\"  Total: {len(available_langs)}\")\n",
    "print(f\"  Exemplos: {available_langs[:10]}...\")\n",
    "\n",
    "print(\"\\nFunção de tradução zero-shot otimizada definida.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c9266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de tradução padrão (usada no fine-tuning)\n",
    "def translate_batch(model, texts, source_lang, target_lang, batch_size=8):\n",
    "    \"\"\"\n",
    "    Traduz um lote de textos usando o modelo.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo de tradução\n",
    "        texts: Lista de textos a traduzir\n",
    "        source_lang: Código do idioma fonte\n",
    "        target_lang: Código do idioma alvo\n",
    "        batch_size: Tamanho do lote\n",
    "        \n",
    "    Returns:\n",
    "        Lista de traduções\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    translations = []\n",
    "    \n",
    "    tokenizer.src_lang = source_lang\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            batch, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True,\n",
    "            max_length=CONFIG[\"max_input_length\"]\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(\n",
    "                **inputs,\n",
    "                forced_bos_token_id=tokenizer.lang_code_to_id[target_lang],\n",
    "                max_length=CONFIG[\"max_target_length\"],\n",
    "                num_beams=5,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        decoded = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "        translations.extend(decoded)\n",
    "        \n",
    "        if (i // batch_size + 1) % 10 == 0:\n",
    "            print(f\"  Traduzidos: {min(i + batch_size, len(texts))}/{len(texts)}\")\n",
    "    \n",
    "    return translations\n",
    "\n",
    "print(\"Função de tradução padrão definida (para fine-tuning).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5cc0b3",
   "metadata": {},
   "source": [
    "### 7.1 Experimento Zero-Shot com Múltiplas Configurações\n",
    "\n",
    "Testamos diferentes configurações de línguas para encontrar a melhor transferência cross-lingual para Tupi Antigo.\n",
    "\n",
    "**Estratégia**: Como o Tupi Antigo não está no mBART, experimentamos:\n",
    "1. Tratá-lo como uma variante do Português (morfologia diferente)\n",
    "2. Usar Espanhol como intermediário (pode capturar padrões latinos diferentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f3afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dados de teste\n",
    "test_portuguese = test_df[\"Português\"].tolist()\n",
    "test_tupi_ref = test_df[\"Tupi Antigo\"].tolist()\n",
    "test_tupi = test_df[\"Tupi Antigo\"].tolist()\n",
    "test_portuguese_ref = test_df[\"Português\"].tolist()\n",
    "\n",
    "print(f\"Conjunto de teste: {len(test_portuguese)} exemplos\")\n",
    "print(f\"\\nExemplos do corpus:\")\n",
    "for i in range(min(3, len(test_portuguese))):\n",
    "    print(f\"  PT: {test_portuguese[i][:60]}...\")\n",
    "    print(f\"  TA: {test_tupi_ref[i][:60]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ee9801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ZERO-SHOT MELHORADO: Português → Tupi Antigo\n",
    "# ============================================================================\n",
    "# Estratégia: Testar múltiplas configurações e usar a melhor\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTO ZERO-SHOT: Português → Tupi Antigo\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Configurações a testar para PT → TA\n",
    "# Hipótese: O modelo pode \"entender\" Tupi como uma língua próxima ao Português\n",
    "configs_pt_ta = [\n",
    "    {\"src\": \"pt_XX\", \"tgt\": \"pt_XX\", \"name\": \"PT→PT (identidade)\", \"beams\": 10},\n",
    "    {\"src\": \"pt_XX\", \"tgt\": \"es_XX\", \"name\": \"PT→ES (espanhol)\", \"beams\": 10},\n",
    "    {\"src\": \"pt_XX\", \"tgt\": \"it_XX\", \"name\": \"PT→IT (italiano)\", \"beams\": 10},\n",
    "    {\"src\": \"pt_XX\", \"tgt\": \"ro_RO\", \"name\": \"PT→RO (romeno)\", \"beams\": 10},\n",
    "    {\"src\": \"pt_XX\", \"tgt\": \"ca_XX\", \"name\": \"PT→CA (catalão)\", \"beams\": 10},\n",
    "]\n",
    "\n",
    "results_pt_ta_configs = {}\n",
    "\n",
    "for config in configs_pt_ta:\n",
    "    print(f\"\\nTestando: {config['name']}\")\n",
    "    try:\n",
    "        translations = translate_batch_zeroshot(\n",
    "            model_zero_shot,\n",
    "            test_portuguese,\n",
    "            source_lang=config[\"src\"],\n",
    "            target_lang=config[\"tgt\"],\n",
    "            num_beams=config[\"beams\"],\n",
    "            length_penalty=1.0,\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "        \n",
    "        metrics = compute_all_metrics(translations, test_tupi_ref)\n",
    "        results_pt_ta_configs[config[\"name\"]] = {\n",
    "            \"translations\": translations,\n",
    "            \"metrics\": metrics,\n",
    "            \"config\": config\n",
    "        }\n",
    "        \n",
    "        print(f\"  BLEU: {metrics['bleu']:.2f} | chrF1: {metrics['chrf1']:.2f} | chrF3: {metrics['chrf3']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Erro: {e}\")\n",
    "\n",
    "# Encontrar a melhor configuração baseada em chrF (melhor para línguas morfologicamente ricas)\n",
    "best_config_pt_ta = max(results_pt_ta_configs.items(), \n",
    "                        key=lambda x: x[1][\"metrics\"][\"chrf1\"])\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"MELHOR CONFIGURAÇÃO PT→TA: {best_config_pt_ta[0]}\")\n",
    "print(f\"  BLEU:  {best_config_pt_ta[1]['metrics']['bleu']:.2f}\")\n",
    "print(f\"  chrF1: {best_config_pt_ta[1]['metrics']['chrf1']:.2f}\")\n",
    "print(f\"  chrF3: {best_config_pt_ta[1]['metrics']['chrf3']:.2f}\")\n",
    "\n",
    "# Usar os melhores resultados\n",
    "translations_pt_ta_zero = best_config_pt_ta[1][\"translations\"]\n",
    "metrics_pt_ta_zero = best_config_pt_ta[1][\"metrics\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bbc75a",
   "metadata": {},
   "source": [
    "### 7.2 Zero-Shot: Tupi Antigo → Português\n",
    "\n",
    "Para a direção inversa, testamos tratar o Tupi Antigo como diferentes línguas fonte conhecidas pelo mBART."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f38eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ZERO-SHOT MELHORADO: Tupi Antigo → Português\n",
    "# ============================================================================\n",
    "# Para TA→PT, a estratégia é diferente: o modelo deve \"entender\" Tupi\n",
    "# e traduzir para Português. Testamos tratar Tupi como diferentes línguas fonte.\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENTO ZERO-SHOT: Tupi Antigo → Português\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Configurações a testar para TA → PT\n",
    "# Hipótese: O modelo pode processar Tupi melhor se tratá-lo como língua similar\n",
    "configs_ta_pt = [\n",
    "    {\"src\": \"pt_XX\", \"tgt\": \"pt_XX\", \"name\": \"PT→PT (identidade)\", \"beams\": 10},\n",
    "    {\"src\": \"es_XX\", \"tgt\": \"pt_XX\", \"name\": \"ES→PT (espanhol fonte)\", \"beams\": 10},\n",
    "    {\"src\": \"it_XX\", \"tgt\": \"pt_XX\", \"name\": \"IT→PT (italiano fonte)\", \"beams\": 10},\n",
    "    {\"src\": \"ro_RO\", \"tgt\": \"pt_XX\", \"name\": \"RO→PT (romeno fonte)\", \"beams\": 10},\n",
    "    {\"src\": \"ca_XX\", \"tgt\": \"pt_XX\", \"name\": \"CA→PT (catalão fonte)\", \"beams\": 10},\n",
    "]\n",
    "\n",
    "results_ta_pt_configs = {}\n",
    "\n",
    "for config in configs_ta_pt:\n",
    "    print(f\"\\nTestando: {config['name']}\")\n",
    "    try:\n",
    "        translations = translate_batch_zeroshot(\n",
    "            model_zero_shot,\n",
    "            test_tupi,\n",
    "            source_lang=config[\"src\"],\n",
    "            target_lang=config[\"tgt\"],\n",
    "            num_beams=config[\"beams\"],\n",
    "            length_penalty=1.0,\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "        \n",
    "        metrics = compute_all_metrics(translations, test_portuguese_ref)\n",
    "        results_ta_pt_configs[config[\"name\"]] = {\n",
    "            \"translations\": translations,\n",
    "            \"metrics\": metrics,\n",
    "            \"config\": config\n",
    "        }\n",
    "        \n",
    "        print(f\"  BLEU: {metrics['bleu']:.2f} | chrF1: {metrics['chrf1']:.2f} | chrF3: {metrics['chrf3']:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Erro: {e}\")\n",
    "\n",
    "# Encontrar a melhor configuração\n",
    "best_config_ta_pt = max(results_ta_pt_configs.items(), \n",
    "                        key=lambda x: x[1][\"metrics\"][\"chrf1\"])\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"MELHOR CONFIGURAÇÃO TA→PT: {best_config_ta_pt[0]}\")\n",
    "print(f\"  BLEU:  {best_config_ta_pt[1]['metrics']['bleu']:.2f}\")\n",
    "print(f\"  chrF1: {best_config_ta_pt[1]['metrics']['chrf1']:.2f}\")\n",
    "print(f\"  chrF3: {best_config_ta_pt[1]['metrics']['chrf3']:.2f}\")\n",
    "\n",
    "# Usar os melhores resultados\n",
    "translations_ta_pt_zero = best_config_ta_pt[1][\"translations\"]\n",
    "metrics_ta_pt_zero = best_config_ta_pt[1][\"metrics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3887f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANÁLISE COMPARATIVA DAS CONFIGURAÇÕES ZERO-SHOT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RESUMO: COMPARAÇÃO DE TODAS AS CONFIGURAÇÕES ZERO-SHOT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n📊 PT → TA (Português → Tupi Antigo):\")\n",
    "print(f\"{'Configuração':<30} {'BLEU':>10} {'chrF1':>10} {'chrF3':>10}\")\n",
    "print(\"-\" * 60)\n",
    "for name, data in sorted(results_pt_ta_configs.items(), key=lambda x: -x[1][\"metrics\"][\"chrf1\"]):\n",
    "    m = data[\"metrics\"]\n",
    "    marker = \" ✓\" if name == best_config_pt_ta[0] else \"\"\n",
    "    print(f\"{name:<30} {m['bleu']:>10.2f} {m['chrf1']:>10.2f} {m['chrf3']:>10.2f}{marker}\")\n",
    "\n",
    "print(\"\\n📊 TA → PT (Tupi Antigo → Português):\")\n",
    "print(f\"{'Configuração':<30} {'BLEU':>10} {'chrF1':>10} {'chrF3':>10}\")\n",
    "print(\"-\" * 60)\n",
    "for name, data in sorted(results_ta_pt_configs.items(), key=lambda x: -x[1][\"metrics\"][\"chrf1\"]):\n",
    "    m = data[\"metrics\"]\n",
    "    marker = \" ✓\" if name == best_config_ta_pt[0] else \"\"\n",
    "    print(f\"{name:<30} {m['bleu']:>10.2f} {m['chrf1']:>10.2f} {m['chrf3']:>10.2f}{marker}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXEMPLOS QUALITATIVOS - MELHORES CONFIGURAÇÕES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "n_examples = min(5, len(test_portuguese))\n",
    "print(\"\\n--- PT → TA ---\")\n",
    "for i in range(n_examples):\n",
    "    print(f\"\\n[{i+1}] Fonte:      {test_portuguese[i]}\")\n",
    "    print(f\"    Referência: {test_tupi_ref[i]}\")\n",
    "    print(f\"    Zero-Shot:  {translations_pt_ta_zero[i]}\")\n",
    "\n",
    "print(\"\\n--- TA → PT ---\")\n",
    "for i in range(n_examples):\n",
    "    print(f\"\\n[{i+1}] Fonte:      {test_tupi[i]}\")\n",
    "    print(f\"    Referência: {test_portuguese_ref[i]}\")\n",
    "    print(f\"    Zero-Shot:  {translations_ta_pt_zero[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237517e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar resultados zero-shot\n",
    "results_zero_shot = {\n",
    "    \"pt_to_ta\": {\n",
    "        \"bleu\": metrics_pt_ta_zero[\"bleu\"],\n",
    "        \"chrf1\": metrics_pt_ta_zero[\"chrf1\"],\n",
    "        \"chrf3\": metrics_pt_ta_zero[\"chrf3\"],\n",
    "        \"bleu_details\": metrics_pt_ta_zero[\"bleu_details\"]\n",
    "    },\n",
    "    \"ta_to_pt\": {\n",
    "        \"bleu\": metrics_ta_pt_zero[\"bleu\"],\n",
    "        \"chrf1\": metrics_ta_pt_zero[\"chrf1\"],\n",
    "        \"chrf3\": metrics_ta_pt_zero[\"chrf3\"],\n",
    "        \"bleu_details\": metrics_ta_pt_zero[\"bleu_details\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Salvar em arquivo JSON\n",
    "with open(os.path.join(CONFIG[\"results_dir\"], \"results_zero_shot.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_zero_shot, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Resultados zero-shot salvos em results/results_zero_shot.json\")\n",
    "\n",
    "# Salvar traduções\n",
    "os.makedirs(os.path.join(CONFIG[\"results_dir\"], \"outputs_zero_shot\"), exist_ok=True)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"source\": test_portuguese,\n",
    "    \"reference\": test_tupi_ref,\n",
    "    \"translation\": translations_pt_ta_zero\n",
    "}).to_csv(os.path.join(CONFIG[\"results_dir\"], \"outputs_zero_shot\", \"pt_to_ta.csv\"), index=False)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"source\": test_tupi,\n",
    "    \"reference\": test_portuguese_ref,\n",
    "    \"translation\": translations_ta_pt_zero\n",
    "}).to_csv(os.path.join(CONFIG[\"results_dir\"], \"outputs_zero_shot\", \"ta_to_pt.csv\"), index=False)\n",
    "\n",
    "print(\"Traduções salvas em results/outputs_zero_shot/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db067c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liberar memória do modelo zero-shot\n",
    "del model_zero_shot\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"Memória liberada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c740bd99",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Fine-Tuning (Few-Shot Learning)\n",
    "\n",
    "No regime **few-shot**, realizamos o ajuste fino (fine-tuning) do modelo mBART usando o corpus de treinamento.\n",
    "\n",
    "### Estratégia de Treinamento\n",
    "\n",
    "1. **LoRA (Low-Rank Adaptation)**: Utilizamos LoRA para reduzir o número de parâmetros treináveis, tornando o fine-tuning mais eficiente\n",
    "2. **Early Stopping**: Interrompemos o treinamento quando a perda de validação para de melhorar\n",
    "3. **Duas direções**: Treinamos modelos separados para PT→TA e TA→PT\n",
    "\n",
    "### Hiperparâmetros\n",
    "- Learning rate: 5e-5\n",
    "- Batch size: 4\n",
    "- Early stopping patience: 3 epochs\n",
    "- LoRA rank: 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4740457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainer(model, tokenizer, train_dataset, val_dataset, output_dir, direction):\n",
    "    \"\"\"\n",
    "    Cria um Seq2SeqTrainer configurado para o fine-tuning.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo a ser treinado\n",
    "        tokenizer: Tokenizador\n",
    "        train_dataset: Dataset de treino tokenizado\n",
    "        val_dataset: Dataset de validação tokenizado\n",
    "        output_dir: Diretório para salvar checkpoints\n",
    "        direction: Direção da tradução ('pt_to_ta' ou 'ta_to_pt')\n",
    "        \n",
    "    Returns:\n",
    "        Trainer configurado\n",
    "    \"\"\"\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "    \n",
    "    def compute_metrics_trainer(eval_preds):\n",
    "        preds, labels = eval_preds\n",
    "        \n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "        \n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        \n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        # Para BLEU\n",
    "        refs = [[l] for l in decoded_labels]\n",
    "        bleu_result = bleu_metric.compute(predictions=decoded_preds, references=refs)\n",
    "        \n",
    "        return {\"bleu\": bleu_result[\"score\"]}\n",
    "    \n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=CONFIG[\"learning_rate\"],\n",
    "        per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "        per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "        num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "        weight_decay=CONFIG[\"weight_decay\"],\n",
    "        warmup_steps=CONFIG[\"warmup_steps\"],\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"bleu\",\n",
    "        greater_is_better=True,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=CONFIG[\"max_target_length\"],\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        logging_steps=50,\n",
    "        save_total_limit=2,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics_trainer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=CONFIG[\"early_stopping_patience\"])]\n",
    "    )\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "print(\"Função de criação do trainer definida.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0509dd62",
   "metadata": {},
   "source": [
    "### 8.1 Fine-Tuning: Português → Tupi Antigo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f704e16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizar datasets para PT -> TA\n",
    "print(\"Tokenizando datasets para PT → TA...\")\n",
    "tokenized_train_pt_ta = dataset_dict[\"train\"].map(preprocess_pt_to_ta, batched=True)\n",
    "tokenized_val_pt_ta = dataset_dict[\"validation\"].map(preprocess_pt_to_ta, batched=True)\n",
    "\n",
    "print(f\"Treino: {len(tokenized_train_pt_ta)} exemplos\")\n",
    "print(f\"Validação: {len(tokenized_val_pt_ta)} exemplos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd9c8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar modelo base para PT -> TA\n",
    "print(\"Carregando modelo base para fine-tuning PT → TA...\")\n",
    "model_pt_ta = MBartForConditionalGeneration.from_pretrained(\n",
    "    CONFIG[\"model_checkpoint\"],\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "\n",
    "# Aplicar LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "model_pt_ta = get_peft_model(model_pt_ta, lora_config)\n",
    "model_pt_ta.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628dd702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar modelo PT -> TA\n",
    "print(\"Iniciando treinamento PT → TA...\")\n",
    "trainer_pt_ta = create_trainer(\n",
    "    model=model_pt_ta,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_train_pt_ta,\n",
    "    val_dataset=tokenized_val_pt_ta,\n",
    "    output_dir=os.path.join(CONFIG[\"models_dir\"], \"pt_to_ta\"),\n",
    "    direction=\"pt_to_ta\"\n",
    ")\n",
    "\n",
    "train_result_pt_ta = trainer_pt_ta.train()\n",
    "\n",
    "print(\"\\n=== Treinamento PT → TA Concluído ===\")\n",
    "print(f\"Épocas: {train_result_pt_ta.metrics.get('epoch', 'N/A')}\")\n",
    "print(f\"Loss final: {train_result_pt_ta.metrics.get('train_loss', 'N/A'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fa2a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar modelo PT -> TA\n",
    "model_pt_ta.save_pretrained(os.path.join(CONFIG[\"models_dir\"], \"pt_to_ta_final\"))\n",
    "tokenizer.save_pretrained(os.path.join(CONFIG[\"models_dir\"], \"pt_to_ta_final\"))\n",
    "print(f\"Modelo PT → TA salvo em {CONFIG['models_dir']}/pt_to_ta_final/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe78c44",
   "metadata": {},
   "source": [
    "### 8.2 Fine-Tuning: Tupi Antigo → Português"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8fa293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liberar memória do modelo anterior\n",
    "del model_pt_ta, trainer_pt_ta\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Tokenizar datasets para TA -> PT\n",
    "print(\"Tokenizando datasets para TA → PT...\")\n",
    "tokenized_train_ta_pt = dataset_dict[\"train\"].map(preprocess_ta_to_pt, batched=True)\n",
    "tokenized_val_ta_pt = dataset_dict[\"validation\"].map(preprocess_ta_to_pt, batched=True)\n",
    "\n",
    "print(f\"Treino: {len(tokenized_train_ta_pt)} exemplos\")\n",
    "print(f\"Validação: {len(tokenized_val_ta_pt)} exemplos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7557fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar modelo base para TA -> PT\n",
    "print(\"Carregando modelo base para fine-tuning TA → PT...\")\n",
    "model_ta_pt = MBartForConditionalGeneration.from_pretrained(\n",
    "    CONFIG[\"model_checkpoint\"],\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "\n",
    "# Aplicar LoRA\n",
    "model_ta_pt = get_peft_model(model_ta_pt, lora_config)\n",
    "model_ta_pt.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0042f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar modelo TA -> PT\n",
    "print(\"Iniciando treinamento TA → PT...\")\n",
    "trainer_ta_pt = create_trainer(\n",
    "    model=model_ta_pt,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_train_ta_pt,\n",
    "    val_dataset=tokenized_val_ta_pt,\n",
    "    output_dir=os.path.join(CONFIG[\"models_dir\"], \"ta_to_pt\"),\n",
    "    direction=\"ta_to_pt\"\n",
    ")\n",
    "\n",
    "train_result_ta_pt = trainer_ta_pt.train()\n",
    "\n",
    "print(\"\\n=== Treinamento TA → PT Concluído ===\")\n",
    "print(f\"Épocas: {train_result_ta_pt.metrics.get('epoch', 'N/A')}\")\n",
    "print(f\"Loss final: {train_result_ta_pt.metrics.get('train_loss', 'N/A'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a735249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar modelo TA -> PT\n",
    "model_ta_pt.save_pretrained(os.path.join(CONFIG[\"models_dir\"], \"ta_to_pt_final\"))\n",
    "tokenizer.save_pretrained(os.path.join(CONFIG[\"models_dir\"], \"ta_to_pt_final\"))\n",
    "print(f\"Modelo TA → PT salvo em {CONFIG['models_dir']}/ta_to_pt_final/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7ef53e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Avaliação dos Modelos Fine-Tuned\n",
    "\n",
    "Agora avaliamos os modelos treinados no conjunto de teste, que nunca foi usado durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b45418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar modelos fine-tuned para avaliação\n",
    "from peft import PeftModel\n",
    "\n",
    "def load_finetuned_model(model_path):\n",
    "    \"\"\"Carrega um modelo fine-tuned com LoRA.\"\"\"\n",
    "    base_model = MBartForConditionalGeneration.from_pretrained(\n",
    "        CONFIG[\"model_checkpoint\"],\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Carregar modelos\n",
    "print(\"Carregando modelos fine-tuned...\")\n",
    "model_pt_ta_ft = load_finetuned_model(os.path.join(CONFIG[\"models_dir\"], \"pt_to_ta_final\"))\n",
    "print(\"  ✓ Modelo PT → TA carregado\")\n",
    "\n",
    "model_ta_pt_ft = load_finetuned_model(os.path.join(CONFIG[\"models_dir\"], \"ta_to_pt_final\"))\n",
    "print(\"  ✓ Modelo TA → PT carregado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ca6018",
   "metadata": {},
   "source": [
    "### 9.1 Avaliação PT → TA (Fine-Tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca39e3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar PT -> TA (fine-tuned)\n",
    "print(\"Traduzindo PT → TA (fine-tuned)...\")\n",
    "translations_pt_ta_ft = translate_batch(\n",
    "    model_pt_ta_ft,\n",
    "    test_portuguese,\n",
    "    source_lang=LANG_CODE,\n",
    "    target_lang=LANG_CODE\n",
    ")\n",
    "\n",
    "metrics_pt_ta_ft = compute_all_metrics(translations_pt_ta_ft, test_tupi_ref)\n",
    "\n",
    "print(\"\\n=== Métricas Fine-Tuned PT → TA ===\")\n",
    "print(f\"  BLEU:  {metrics_pt_ta_ft['bleu']:.2f}\")\n",
    "print(f\"  chrF1: {metrics_pt_ta_ft['chrf1']:.2f}\")\n",
    "print(f\"  chrF3: {metrics_pt_ta_ft['chrf3']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beb750a",
   "metadata": {},
   "source": [
    "### 9.2 Avaliação TA → PT (Fine-Tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2b7bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar TA -> PT (fine-tuned)\n",
    "print(\"Traduzindo TA → PT (fine-tuned)...\")\n",
    "translations_ta_pt_ft = translate_batch(\n",
    "    model_ta_pt_ft,\n",
    "    test_tupi,\n",
    "    source_lang=LANG_CODE,\n",
    "    target_lang=LANG_CODE\n",
    ")\n",
    "\n",
    "metrics_ta_pt_ft = compute_all_metrics(translations_ta_pt_ft, test_portuguese_ref)\n",
    "\n",
    "print(\"\\n=== Métricas Fine-Tuned TA → PT ===\")\n",
    "print(f\"  BLEU:  {metrics_ta_pt_ft['bleu']:.2f}\")\n",
    "print(f\"  chrF1: {metrics_ta_pt_ft['chrf1']:.2f}\")\n",
    "print(f\"  chrF3: {metrics_ta_pt_ft['chrf3']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac903b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar resultados few-shot\n",
    "results_few_shot = {\n",
    "    \"pt_to_ta\": {\n",
    "        \"bleu\": metrics_pt_ta_ft[\"bleu\"],\n",
    "        \"chrf1\": metrics_pt_ta_ft[\"chrf1\"],\n",
    "        \"chrf3\": metrics_pt_ta_ft[\"chrf3\"],\n",
    "        \"bleu_details\": metrics_pt_ta_ft[\"bleu_details\"]\n",
    "    },\n",
    "    \"ta_to_pt\": {\n",
    "        \"bleu\": metrics_ta_pt_ft[\"bleu\"],\n",
    "        \"chrf1\": metrics_ta_pt_ft[\"chrf1\"],\n",
    "        \"chrf3\": metrics_ta_pt_ft[\"chrf3\"],\n",
    "        \"bleu_details\": metrics_ta_pt_ft[\"bleu_details\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Salvar em arquivo JSON\n",
    "with open(os.path.join(CONFIG[\"results_dir\"], \"results_few_shot.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_few_shot, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Resultados few-shot salvos em results/results_few_shot.json\")\n",
    "\n",
    "# Salvar traduções\n",
    "os.makedirs(os.path.join(CONFIG[\"results_dir\"], \"outputs_few_shot\"), exist_ok=True)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"source\": test_portuguese,\n",
    "    \"reference\": test_tupi_ref,\n",
    "    \"translation\": translations_pt_ta_ft\n",
    "}).to_csv(os.path.join(CONFIG[\"results_dir\"], \"outputs_few_shot\", \"pt_to_ta.csv\"), index=False)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"source\": test_tupi,\n",
    "    \"reference\": test_portuguese_ref,\n",
    "    \"translation\": translations_ta_pt_ft\n",
    "}).to_csv(os.path.join(CONFIG[\"results_dir\"], \"outputs_few_shot\", \"ta_to_pt.csv\"), index=False)\n",
    "\n",
    "print(\"Traduções salvas em results/outputs_few_shot/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec88ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Comparação: Zero-Shot vs Fine-Tuned\n",
    "\n",
    "### Matriz Comparativa de Métricas\n",
    "\n",
    "Comparamos os resultados obtidos nos dois regimes (zero-shot e few-shot) para ambas as direções de tradução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc786c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar matriz comparativa\n",
    "comparison_data = {\n",
    "    \"Direção\": [\"PT → TA\", \"PT → TA\", \"TA → PT\", \"TA → PT\"],\n",
    "    \"Modo\": [\"Zero-Shot\", \"Fine-Tuned\", \"Zero-Shot\", \"Fine-Tuned\"],\n",
    "    \"BLEU\": [\n",
    "        metrics_pt_ta_zero[\"bleu\"],\n",
    "        metrics_pt_ta_ft[\"bleu\"],\n",
    "        metrics_ta_pt_zero[\"bleu\"],\n",
    "        metrics_ta_pt_ft[\"bleu\"]\n",
    "    ],\n",
    "    \"chrF1\": [\n",
    "        metrics_pt_ta_zero[\"chrf1\"],\n",
    "        metrics_pt_ta_ft[\"chrf1\"],\n",
    "        metrics_ta_pt_zero[\"chrf1\"],\n",
    "        metrics_ta_pt_ft[\"chrf1\"]\n",
    "    ],\n",
    "    \"chrF3\": [\n",
    "        metrics_pt_ta_zero[\"chrf3\"],\n",
    "        metrics_pt_ta_ft[\"chrf3\"],\n",
    "        metrics_ta_pt_zero[\"chrf3\"],\n",
    "        metrics_ta_pt_ft[\"chrf3\"]\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"=== Matriz Comparativa de Métricas ===\\n\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Calcular melhorias\n",
    "print(\"\\n\\n=== Melhoria com Fine-Tuning ===\")\n",
    "print(f\"\\nPT → TA:\")\n",
    "print(f\"  BLEU:  {metrics_pt_ta_ft['bleu'] - metrics_pt_ta_zero['bleu']:+.2f} pontos\")\n",
    "print(f\"  chrF1: {metrics_pt_ta_ft['chrf1'] - metrics_pt_ta_zero['chrf1']:+.2f} pontos\")\n",
    "print(f\"  chrF3: {metrics_pt_ta_ft['chrf3'] - metrics_pt_ta_zero['chrf3']:+.2f} pontos\")\n",
    "\n",
    "print(f\"\\nTA → PT:\")\n",
    "print(f\"  BLEU:  {metrics_ta_pt_ft['bleu'] - metrics_ta_pt_zero['bleu']:+.2f} pontos\")\n",
    "print(f\"  chrF1: {metrics_ta_pt_ft['chrf1'] - metrics_ta_pt_zero['chrf1']:+.2f} pontos\")\n",
    "print(f\"  chrF3: {metrics_ta_pt_ft['chrf3'] - metrics_ta_pt_zero['chrf3']:+.2f} pontos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d535f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização gráfica\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    metrics_names = [\"BLEU\", \"chrF1\", \"chrF3\"]\n",
    "    x = np.arange(2)\n",
    "    width = 0.35\n",
    "    \n",
    "    for idx, metric in enumerate(metrics_names):\n",
    "        ax = axes[idx]\n",
    "        zero_shot = [metrics_pt_ta_zero[metric.lower()], metrics_ta_pt_zero[metric.lower()]]\n",
    "        fine_tuned = [metrics_pt_ta_ft[metric.lower()], metrics_ta_pt_ft[metric.lower()]]\n",
    "        \n",
    "        bars1 = ax.bar(x - width/2, zero_shot, width, label='Zero-Shot', color='#ff7f0e')\n",
    "        bars2 = ax.bar(x + width/2, fine_tuned, width, label='Fine-Tuned', color='#1f77b4')\n",
    "        \n",
    "        ax.set_xlabel('Direção')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.set_title(f'{metric}')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(['PT → TA', 'TA → PT'])\n",
    "        ax.legend()\n",
    "        ax.set_ylim(0, 100)\n",
    "        \n",
    "        # Adicionar valores nas barras\n",
    "        for bar in bars1 + bars2:\n",
    "            height = bar.get_height()\n",
    "            ax.annotate(f'{height:.1f}',\n",
    "                       xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                       xytext=(0, 3),\n",
    "                       textcoords=\"offset points\",\n",
    "                       ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.suptitle('Comparação: Zero-Shot vs Fine-Tuned', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG[\"results_dir\"], \"comparison_chart.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"\\nGráfico salvo em {CONFIG['results_dir']}/comparison_chart.png\")\n",
    "except ImportError:\n",
    "    print(\"matplotlib não disponível para visualização gráfica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bc6ac3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Exemplos Qualitativos\n",
    "\n",
    "Analisamos algumas traduções para entender qualitativamente o desempenho dos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e527275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplos qualitativos PT -> TA\n",
    "print(\"=\" * 80)\n",
    "print(\"EXEMPLOS QUALITATIVOS: Português → Tupi Antigo\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "n_examples = min(10, len(test_portuguese))\n",
    "for i in range(n_examples):\n",
    "    print(f\"\\n--- Exemplo {i+1} ---\")\n",
    "    print(f\"📝 Fonte (PT):      {test_portuguese[i]}\")\n",
    "    print(f\"✅ Referência (TA): {test_tupi_ref[i]}\")\n",
    "    print(f\"🔴 Zero-Shot:       {translations_pt_ta_zero[i]}\")\n",
    "    print(f\"🟢 Fine-Tuned:      {translations_pt_ta_ft[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff541535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplos qualitativos TA -> PT\n",
    "print(\"=\" * 80)\n",
    "print(\"EXEMPLOS QUALITATIVOS: Tupi Antigo → Português\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(n_examples):\n",
    "    print(f\"\\n--- Exemplo {i+1} ---\")\n",
    "    print(f\"📝 Fonte (TA):      {test_tupi[i]}\")\n",
    "    print(f\"✅ Referência (PT): {test_portuguese_ref[i]}\")\n",
    "    print(f\"🔴 Zero-Shot:       {translations_ta_pt_zero[i]}\")\n",
    "    print(f\"🟢 Fine-Tuned:      {translations_ta_pt_ft[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dc9314",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Discussão e Limitações\n",
    "\n",
    "### 12.1 Análise dos Resultados\n",
    "\n",
    "#### Zero-Shot\n",
    "- O modelo mBART, apesar de multilíngue, não possui conhecimento prévio do Tupi Antigo\n",
    "- As traduções zero-shot tendem a ser em português ou em outros idiomas do vocabulário do modelo\n",
    "- As métricas baixas refletem essa limitação fundamental\n",
    "\n",
    "#### Fine-Tuned\n",
    "- O fine-tuning com LoRA permite adaptar o modelo ao par linguístico Português-Tupi Antigo\n",
    "- Mesmo com um corpus pequeno, observamos melhoria significativa nas métricas\n",
    "- O modelo aprende padrões específicos da língua Tupi, incluindo sua morfologia\n",
    "\n",
    "### 12.2 Limitações\n",
    "\n",
    "1. **Tamanho do Corpus**: Corpora de baixo recurso limitam o aprendizado do modelo\n",
    "\n",
    "2. **Tokenização**: O tokenizador do mBART não foi otimizado para Tupi Antigo, podendo segmentar palavras de forma subótima\n",
    "\n",
    "3. **Código de Idioma**: Usamos pt_XX como proxy, o que não é ideal\n",
    "\n",
    "4. **Variação Ortográfica**: O Tupi Antigo possui variações históricas que podem confundir o modelo\n",
    "\n",
    "5. **Avaliação Automática**: Métricas como BLEU podem não capturar adequadamente a qualidade semântica\n",
    "\n",
    "### 12.3 Trabalhos Futuros\n",
    "\n",
    "1. Expandir o corpus paralelo\n",
    "2. Desenvolver um tokenizador específico para Tupi Antigo\n",
    "3. Explorar técnicas de data augmentation\n",
    "4. Avaliar outros modelos (NLLB-200, mT5)\n",
    "5. Realizar avaliação humana das traduções"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7d6672",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. Conclusão\n",
    "\n",
    "Neste EP, implementamos e avaliamos tradutores automáticos para o par linguístico **Português ↔ Tupi Antigo** em dois regimes:\n",
    "\n",
    "### Principais Resultados\n",
    "\n",
    "1. **Zero-Shot**: O modelo mBART pré-treinado não consegue traduzir adequadamente para/de Tupi Antigo sem fine-tuning, confirmando a necessidade de adaptação para línguas de baixo recurso.\n",
    "\n",
    "2. **Fine-Tuned**: Com fine-tuning usando LoRA, observamos melhorias significativas em todas as métricas, demonstrando que mesmo corpora pequenos podem ser úteis para adaptar modelos multilíngues.\n",
    "\n",
    "3. **Direção da Tradução**: A tradução TA → PT tende a ter melhores resultados, possivelmente porque o português é uma língua bem representada no modelo base.\n",
    "\n",
    "### Arquivos Gerados\n",
    "\n",
    "- `results/results_zero_shot.json`: Métricas do regime zero-shot\n",
    "- `results/results_few_shot.json`: Métricas do regime few-shot\n",
    "- `results/outputs_zero_shot/`: Traduções geradas (zero-shot)\n",
    "- `results/outputs_few_shot/`: Traduções geradas (fine-tuned)\n",
    "- `models/pt_to_ta_final/`: Modelo fine-tuned PT → TA\n",
    "- `models/ta_to_pt_final/`: Modelo fine-tuned TA → PT\n",
    "- `data/train.csv`, `data/val.csv`, `data/test.csv`: Divisões do corpus\n",
    "\n",
    "---\n",
    "\n",
    "**MAC0508 — Introdução ao Processamento de Língua Natural**  \n",
    "**EP2 — Tradução Automática de Baixo Recurso**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8319351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo final dos resultados\n",
    "print(\"=\" * 60)\n",
    "print(\"RESUMO FINAL DOS RESULTADOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n📊 MÉTRICAS FINAIS\\n\")\n",
    "print(f\"{'Cenário':<25} {'BLEU':>10} {'chrF1':>10} {'chrF3':>10}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'PT→TA Zero-Shot':<25} {metrics_pt_ta_zero['bleu']:>10.2f} {metrics_pt_ta_zero['chrf1']:>10.2f} {metrics_pt_ta_zero['chrf3']:>10.2f}\")\n",
    "print(f\"{'PT→TA Fine-Tuned':<25} {metrics_pt_ta_ft['bleu']:>10.2f} {metrics_pt_ta_ft['chrf1']:>10.2f} {metrics_pt_ta_ft['chrf3']:>10.2f}\")\n",
    "print(f\"{'TA→PT Zero-Shot':<25} {metrics_ta_pt_zero['bleu']:>10.2f} {metrics_ta_pt_zero['chrf1']:>10.2f} {metrics_ta_pt_zero['chrf3']:>10.2f}\")\n",
    "print(f\"{'TA→PT Fine-Tuned':<25} {metrics_ta_pt_ft['bleu']:>10.2f} {metrics_ta_pt_ft['chrf1']:>10.2f} {metrics_ta_pt_ft['chrf3']:>10.2f}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n✅ Notebook executado com sucesso!\")\n",
    "print(f\"📁 Resultados salvos em: {CONFIG['results_dir']}/\")\n",
    "print(f\"🤖 Modelos salvos em: {CONFIG['models_dir']}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
