{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a890932b",
   "metadata": {},
   "source": [
    "## 1. Configuração do Ambiente\n",
    "\n",
    "### Instalação de Dependências\n",
    "Primeiro, garantimos que todas as bibliotecas necessárias estejam instaladas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4c029e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Instalação das dependências (executar apenas uma vez)\n",
    "# !pip install transformers datasets evaluate sacrebleu pandas openpyxl torch peft sentencepiece\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Imports principais\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Transformers e Datasets\n",
    "from transformers import (\n",
    "    MBart50TokenizerFast,\n",
    "    MBartForConditionalGeneration,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Avaliação\n",
    "import evaluate\n",
    "\n",
    "# PEFT para LoRA (opcional)\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Configuração do dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memória GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f6e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações globais do projeto\n",
    "CONFIG = {\n",
    "    # Modelo\n",
    "    \"model_checkpoint\": \"facebook/mbart-large-50-many-to-many-mmt\",\n",
    "    \n",
    "    # Tokenização\n",
    "    \"max_input_length\": 128,\n",
    "    \"max_target_length\": 128,\n",
    "    \n",
    "    # Treinamento\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"batch_size\": 4,\n",
    "    \"num_epochs\": 10,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_steps\": 100,\n",
    "    \n",
    "    # Early stopping\n",
    "    \"early_stopping_patience\": 3,\n",
    "    \n",
    "    # LoRA\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \n",
    "    # Caminhos\n",
    "    \"data_path\": \"./data.xlsx\",\n",
    "    \"results_dir\": \"./results\",\n",
    "    \"models_dir\": \"./models\",\n",
    "    \n",
    "    # Seed para reprodutibilidade\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "# Criar diretórios\n",
    "os.makedirs(CONFIG[\"results_dir\"], exist_ok=True)\n",
    "os.makedirs(CONFIG[\"models_dir\"], exist_ok=True)\n",
    "\n",
    "print(\"Configurações carregadas:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d9f375",
   "metadata": {},
   "source": [
    "## 2. Justificativa do Modelo Escolhido\n",
    "\n",
    "### Por que mBART-50?\n",
    "\n",
    "Escolhemos o modelo **facebook/mbart-large-50-many-to-many-mmt** pelos seguintes motivos:\n",
    "\n",
    "1. **Arquitetura Encoder-Decoder**: O mBART utiliza a arquitetura Transformer completa (encoder-decoder), ideal para tarefas de tradução automática, diferente de modelos apenas decoder (GPT) ou apenas encoder (BERT).\n",
    "\n",
    "2. **Pré-treinamento Multilíngue**: O modelo foi pré-treinado em 50 idiomas, incluindo o Português. Embora não inclua Tupi Antigo, o conhecimento multilíngue pode ajudar na transferência de padrões linguísticos.\n",
    "\n",
    "3. **Many-to-Many**: Esta variante permite tradução entre qualquer par de idiomas suportados, facilitando a adaptação para novos pares linguísticos.\n",
    "\n",
    "4. **Suporte a Línguas de Baixo Recurso**: O mBART foi projetado especificamente para cenários de baixo recurso, onde há poucos dados de treinamento disponíveis.\n",
    "\n",
    "5. **Fine-tuning Eficiente**: Com técnicas como LoRA (Low-Rank Adaptation), podemos fazer fine-tuning eficiente mesmo com recursos computacionais limitados.\n",
    "\n",
    "### Alternativas Consideradas\n",
    "\n",
    "| Modelo | Prós | Contras |\n",
    "|--------|------|---------|\n",
    "| **mBART-50** | Multilíngue, encoder-decoder, bom para baixo recurso | Grande (1.2GB), requer GPU |\n",
    "| **NLLB-200** | 200 idiomas, otimizado para tradução | Muito grande, pode ser lento |\n",
    "| **mT5** | Flexível, multilíngue | Não específico para tradução |\n",
    "| **T5** | Leve, rápido | Focado em inglês |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab89da7c",
   "metadata": {},
   "source": [
    "## 3. Explicação Matemática das Métricas\n",
    "\n",
    "### 3.1 BLEU (Bilingual Evaluation Understudy)\n",
    "\n",
    "O BLEU mede a similaridade entre uma tradução candidata e uma ou mais traduções de referência usando n-gramas.\n",
    "\n",
    "**Fórmula:**\n",
    "\n",
    "$$\\text{BLEU} = BP \\cdot \\exp\\left(\\sum_{n=1}^{N} w_n \\log p_n\\right)$$\n",
    "\n",
    "Onde:\n",
    "- $p_n$ é a precisão do n-grama: $p_n = \\frac{\\text{n-gramas correspondentes}}{\\text{total de n-gramas na candidata}}$\n",
    "- $w_n$ é o peso para cada n-grama (geralmente $w_n = 1/N$)\n",
    "- $BP$ é a penalidade de brevidade:\n",
    "\n",
    "$$BP = \\begin{cases} 1 & \\text{se } c > r \\\\ e^{1-r/c} & \\text{se } c \\leq r \\end{cases}$$\n",
    "\n",
    "Onde $c$ é o comprimento da candidata e $r$ é o comprimento da referência.\n",
    "\n",
    "### 3.2 chrF (Character-level F-score)\n",
    "\n",
    "O chrF utiliza n-gramas de **caracteres** ao invés de palavras, sendo mais robusto para línguas morfologicamente ricas.\n",
    "\n",
    "**Fórmula:**\n",
    "\n",
    "$$\\text{chrF}_\\beta = (1 + \\beta^2) \\cdot \\frac{\\text{chrP} \\cdot \\text{chrR}}{\\beta^2 \\cdot \\text{chrP} + \\text{chrR}}$$\n",
    "\n",
    "Onde:\n",
    "- $\\text{chrP}$ = Precisão de n-gramas de caracteres\n",
    "- $\\text{chrR}$ = Recall de n-gramas de caracteres\n",
    "- $\\beta$ = Peso do recall (chrF1: $\\beta=1$, chrF3: $\\beta=3$)\n",
    "\n",
    "**chrF1** ($\\beta=1$): Dá peso igual para precisão e recall.\n",
    "\n",
    "**chrF3** ($\\beta=3$): Dá mais peso ao recall, útil quando queremos capturar mais do conteúdo da referência.\n",
    "\n",
    "### Por que chrF é importante para Tupi Antigo?\n",
    "\n",
    "O Tupi Antigo possui morfologia complexa com prefixos, sufixos e variações ortográficas históricas. Métricas baseadas em caracteres capturam melhor essas nuances do que métricas baseadas em palavras.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a1f64a",
   "metadata": {},
   "source": [
    "## 4. Carregamento e Preparação dos Dados\n",
    "\n",
    "### 4.1 Leitura do Corpus\n",
    "\n",
    "O corpus paralelo Português ↔ Tupi Antigo está armazenado em `data.xlsx` com as colunas:\n",
    "- **Português**: Frases em português\n",
    "- **Tupi Antigo**: Traduções correspondentes em Tupi Antigo\n",
    "\n",
    "**Importante**: Preservamos acentos, diacríticos e grafia histórica do Tupi Antigo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031e902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura do corpus\n",
    "df = pd.read_excel(CONFIG[\"data_path\"])\n",
    "\n",
    "print(f\"Corpus carregado: {len(df)} pares de frases\")\n",
    "print(f\"\\nColunas: {list(df.columns)}\")\n",
    "print(f\"\\nPrimeiras 5 linhas:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8a8a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar informações do dataset\n",
    "print(\"Informações do DataFrame:\")\n",
    "print(df.info())\n",
    "print(f\"\\nValores nulos:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nEstatísticas de comprimento (caracteres):\")\n",
    "df['len_pt'] = df['Português'].astype(str).str.len()\n",
    "df['len_ta'] = df['Tupi Antigo'].astype(str).str.len()\n",
    "print(df[['len_pt', 'len_ta']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af7ec89",
   "metadata": {},
   "source": [
    "### 4.2 Limpeza e Normalização Mínima\n",
    "\n",
    "Realizamos apenas limpeza mínima para preservar as características linguísticas do Tupi Antigo:\n",
    "\n",
    "1. **Remoção de espaços extras** (início, fim, múltiplos espaços)\n",
    "2. **Remoção de caracteres invisíveis** (zero-width spaces, etc.)\n",
    "3. **NÃO removemos**: acentos, diacríticos, maiúsculas/minúsculas\n",
    "\n",
    "**Justificativa**: O Tupi Antigo possui variações ortográficas históricas e símbolos especiais que carregam significado linguístico. Uma normalização agressiva poderia remover informações importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453d6f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Limpeza mínima do texto, preservando acentos e diacríticos.\n",
    "    \n",
    "    Args:\n",
    "        text: Texto a ser limpo\n",
    "        \n",
    "    Returns:\n",
    "        Texto limpo\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Remover caracteres invisíveis (zero-width spaces, etc.)\n",
    "    text = re.sub(r'[\\u200b\\u200c\\u200d\\ufeff]', '', text)\n",
    "    \n",
    "    # Remover espaços extras (múltiplos espaços -> um espaço)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remover espaços no início e fim\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Aplicar limpeza\n",
    "df['Português'] = df['Português'].apply(clean_text)\n",
    "df['Tupi Antigo'] = df['Tupi Antigo'].apply(clean_text)\n",
    "\n",
    "# Remover linhas vazias\n",
    "df = df[(df['Português'] != '') & (df['Tupi Antigo'] != '')]\n",
    "\n",
    "# Remover colunas auxiliares de comprimento\n",
    "df = df.drop(columns=['len_pt', 'len_ta'], errors='ignore')\n",
    "\n",
    "print(f\"Após limpeza: {len(df)} pares de frases\")\n",
    "print(\"\\nExemplos após limpeza:\")\n",
    "for i in range(min(3, len(df))):\n",
    "    print(f\"\\n[{i+1}] PT: {df.iloc[i]['Português']}\")\n",
    "    print(f\"    TA: {df.iloc[i]['Tupi Antigo']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef89123c",
   "metadata": {},
   "source": [
    "### 4.3 Divisão do Corpus\n",
    "\n",
    "Dividimos o corpus em três conjuntos:\n",
    "- **Treino (70%)**: Para fine-tuning do modelo\n",
    "- **Validação (15%)**: Para early stopping e seleção de hiperparâmetros\n",
    "- **Teste (15%)**: Para avaliação final (nunca usado durante treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c8f07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Definir seed para reprodutibilidade\n",
    "np.random.seed(CONFIG[\"seed\"])\n",
    "\n",
    "# Primeira divisão: 70% treino, 30% (validação + teste)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.30, random_state=CONFIG[\"seed\"])\n",
    "\n",
    "# Segunda divisão: 50% validação, 50% teste (do temp_df)\n",
    "# Isso resulta em 15% validação e 15% teste do total\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.50, random_state=CONFIG[\"seed\"])\n",
    "\n",
    "print(f\"Divisão do corpus:\")\n",
    "print(f\"  Treino:     {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Validação:  {len(val_df)} ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Teste:      {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Total:      {len(df)}\")\n",
    "\n",
    "# Reset dos índices\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb83f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar os subconjuntos para uso posterior\n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "\n",
    "train_df.to_csv(\"./data/train.csv\", index=False)\n",
    "val_df.to_csv(\"./data/val.csv\", index=False)\n",
    "test_df.to_csv(\"./data/test.csv\", index=False)\n",
    "\n",
    "print(\"Subconjuntos salvos em ./data/\")\n",
    "print(f\"  train.csv: {len(train_df)} exemplos\")\n",
    "print(f\"  val.csv:   {len(val_df)} exemplos\")\n",
    "print(f\"  test.csv:  {len(test_df)} exemplos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f54234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter para Datasets do Hugging Face\n",
    "def df_to_dataset(df):\n",
    "    \"\"\"Converte DataFrame para Dataset do Hugging Face.\"\"\"\n",
    "    return Dataset.from_dict({\n",
    "        \"portuguese\": df[\"Português\"].tolist(),\n",
    "        \"tupi\": df[\"Tupi Antigo\"].tolist()\n",
    "    })\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": df_to_dataset(train_df),\n",
    "    \"validation\": df_to_dataset(val_df),\n",
    "    \"test\": df_to_dataset(test_df)\n",
    "})\n",
    "\n",
    "print(\"Dataset Hugging Face criado:\")\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3140e7a",
   "metadata": {},
   "source": [
    "## 5. Configuração do Modelo e Tokenizador\n",
    "\n",
    "### Tratamento do Tupi Antigo no mBART\n",
    "\n",
    "O mBART não possui código de idioma nativo para Tupi Antigo. Utilizamos uma estratégia de adaptação:\n",
    "\n",
    "1. Usamos o código de idioma do Português (`pt_XX`) como proxy para ambos os idiomas\n",
    "2. Adicionamos prefixos textuais nas entradas para indicar a direção da tradução\n",
    "3. O modelo aprende a associar esses padrões durante o fine-tuning\n",
    "\n",
    "**Nota**: Esta é uma limitação do cenário de baixo recurso. Em um cenário ideal, teríamos um tokenizador e código de idioma específico para Tupi Antigo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a918f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar tokenizador\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(CONFIG[\"model_checkpoint\"])\n",
    "\n",
    "# Configuração de idiomas\n",
    "# Usamos pt_XX como proxy para ambos (PT e Tupi Antigo)\n",
    "# O modelo aprenderá a distinção através dos prefixos e do fine-tuning\n",
    "LANG_CODE = \"pt_XX\"\n",
    "\n",
    "print(f\"Tokenizador carregado: {CONFIG['model_checkpoint']}\")\n",
    "print(f\"Vocabulário: {tokenizer.vocab_size} tokens\")\n",
    "print(f\"Código de idioma usado: {LANG_CODE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6528a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções de pré-processamento para cada direção de tradução\n",
    "\n",
    "def preprocess_pt_to_ta(examples):\n",
    "    \"\"\"Pré-processa para tradução Português -> Tupi Antigo.\"\"\"\n",
    "    inputs = examples[\"portuguese\"]\n",
    "    targets = examples[\"tupi\"]\n",
    "    \n",
    "    tokenizer.src_lang = LANG_CODE\n",
    "    tokenizer.tgt_lang = LANG_CODE\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=CONFIG[\"max_input_length\"], \n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    labels = tokenizer(\n",
    "        text_target=targets,\n",
    "        max_length=CONFIG[\"max_target_length\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_ta_to_pt(examples):\n",
    "    \"\"\"Pré-processa para tradução Tupi Antigo -> Português.\"\"\"\n",
    "    inputs = examples[\"tupi\"]\n",
    "    targets = examples[\"portuguese\"]\n",
    "    \n",
    "    tokenizer.src_lang = LANG_CODE\n",
    "    tokenizer.tgt_lang = LANG_CODE\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs, \n",
    "        max_length=CONFIG[\"max_input_length\"], \n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    labels = tokenizer(\n",
    "        text_target=targets,\n",
    "        max_length=CONFIG[\"max_target_length\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "print(\"Funções de pré-processamento definidas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953c99a9",
   "metadata": {},
   "source": [
    "## 6. Configuração das Métricas de Avaliação\n",
    "\n",
    "Implementamos as métricas conforme especificado no enunciado:\n",
    "- **BLEU**: Usando SacreBLEU para resultados reproduzíveis\n",
    "- **chrF1**: F-score de caracteres com β=1\n",
    "- **chrF3**: F-score de caracteres com β=3 (mais peso no recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66d21a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar métricas\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "chrf_metric = evaluate.load(\"chrf\")\n",
    "\n",
    "def compute_all_metrics(predictions, references):\n",
    "    \"\"\"\n",
    "    Calcula todas as métricas de avaliação.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Lista de traduções geradas\n",
    "        references: Lista de traduções de referência\n",
    "        \n",
    "    Returns:\n",
    "        Dicionário com todas as métricas\n",
    "    \"\"\"\n",
    "    # BLEU espera referências como lista de listas\n",
    "    refs_for_bleu = [[ref] for ref in references]\n",
    "    \n",
    "    # BLEU\n",
    "    bleu_result = bleu_metric.compute(predictions=predictions, references=refs_for_bleu)\n",
    "    \n",
    "    # chrF1 (beta=1)\n",
    "    chrf1_result = chrf_metric.compute(\n",
    "        predictions=predictions, \n",
    "        references=refs_for_bleu,\n",
    "        char_order=6,\n",
    "        word_order=0,\n",
    "        beta=1\n",
    "    )\n",
    "    \n",
    "    # chrF3 (beta=3)\n",
    "    chrf3_result = chrf_metric.compute(\n",
    "        predictions=predictions, \n",
    "        references=refs_for_bleu,\n",
    "        char_order=6,\n",
    "        word_order=0,\n",
    "        beta=3\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"bleu\": bleu_result[\"score\"],\n",
    "        \"chrf1\": chrf1_result[\"score\"],\n",
    "        \"chrf3\": chrf3_result[\"score\"],\n",
    "        \"bleu_details\": {\n",
    "            \"precisions\": bleu_result[\"precisions\"],\n",
    "            \"brevity_penalty\": bleu_result[\"bp\"],\n",
    "            \"length_ratio\": bleu_result[\"sys_len\"] / bleu_result[\"ref_len\"] if bleu_result[\"ref_len\"] > 0 else 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"Métricas carregadas e configuradas:\")\n",
    "print(\"  - BLEU (SacreBLEU)\")\n",
    "print(\"  - chrF1 (β=1)\")\n",
    "print(\"  - chrF3 (β=3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e82aa9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Tradução Zero-Shot\n",
    "\n",
    "No regime **zero-shot**, utilizamos o modelo pré-treinado diretamente, sem qualquer fine-tuning no corpus Português-Tupi Antigo.\n",
    "\n",
    "**Expectativa**: Como o mBART não foi treinado em Tupi Antigo, esperamos resultados limitados. O modelo pode:\n",
    "- Gerar texto em português (ignorando a tarefa)\n",
    "- Gerar texto em outro idioma próximo\n",
    "- Gerar traduções parcialmente corretas baseadas em padrões multilíngues\n",
    "\n",
    "Esta baseline será comparada com o modelo fine-tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fb78c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar modelo para zero-shot\n",
    "print(\"Carregando modelo mBART para zero-shot...\")\n",
    "model_zero_shot = MBartForConditionalGeneration.from_pretrained(\n",
    "    CONFIG[\"model_checkpoint\"],\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "model_zero_shot = model_zero_shot.to(device)\n",
    "model_zero_shot.eval()\n",
    "\n",
    "print(f\"Modelo carregado: {CONFIG['model_checkpoint']}\")\n",
    "print(f\"Parâmetros: {sum(p.numel() for p in model_zero_shot.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd27d6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_batch(model, texts, source_lang, target_lang, batch_size=8):\n",
    "    \"\"\"\n",
    "    Traduz um lote de textos usando o modelo.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo de tradução\n",
    "        texts: Lista de textos a traduzir\n",
    "        source_lang: Código do idioma fonte\n",
    "        target_lang: Código do idioma alvo\n",
    "        batch_size: Tamanho do lote\n",
    "        \n",
    "    Returns:\n",
    "        Lista de traduções\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    translations = []\n",
    "    \n",
    "    tokenizer.src_lang = source_lang\n",
    "    \n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            batch, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True,\n",
    "            max_length=CONFIG[\"max_input_length\"]\n",
    "        ).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(\n",
    "                **inputs,\n",
    "                forced_bos_token_id=tokenizer.lang_code_to_id[target_lang],\n",
    "                max_length=CONFIG[\"max_target_length\"],\n",
    "                num_beams=5,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        decoded = tokenizer.batch_decode(generated, skip_special_tokens=True)\n",
    "        translations.extend(decoded)\n",
    "        \n",
    "        if (i // batch_size + 1) % 10 == 0:\n",
    "            print(f\"  Traduzidos: {min(i + batch_size, len(texts))}/{len(texts)}\")\n",
    "    \n",
    "    return translations\n",
    "\n",
    "print(\"Função de tradução em lote definida.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5cc0b3",
   "metadata": {},
   "source": [
    "### 7.1 Zero-Shot: Português → Tupi Antigo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f3afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot PT -> TA\n",
    "print(\"Traduzindo PT -> TA (zero-shot)...\")\n",
    "test_portuguese = test_df[\"Português\"].tolist()\n",
    "test_tupi_ref = test_df[\"Tupi Antigo\"].tolist()\n",
    "\n",
    "translations_pt_ta_zero = translate_batch(\n",
    "    model_zero_shot, \n",
    "    test_portuguese, \n",
    "    source_lang=LANG_CODE,\n",
    "    target_lang=LANG_CODE\n",
    ")\n",
    "\n",
    "print(f\"\\nTraduzidas {len(translations_pt_ta_zero)} frases.\")\n",
    "\n",
    "# Calcular métricas\n",
    "metrics_pt_ta_zero = compute_all_metrics(translations_pt_ta_zero, test_tupi_ref)\n",
    "\n",
    "print(\"\\n=== Métricas Zero-Shot PT → TA ===\")\n",
    "print(f\"  BLEU:  {metrics_pt_ta_zero['bleu']:.2f}\")\n",
    "print(f\"  chrF1: {metrics_pt_ta_zero['chrf1']:.2f}\")\n",
    "print(f\"  chrF3: {metrics_pt_ta_zero['chrf3']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bbc75a",
   "metadata": {},
   "source": [
    "### 7.2 Zero-Shot: Tupi Antigo → Português"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f38eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot TA -> PT\n",
    "print(\"Traduzindo TA -> PT (zero-shot)...\")\n",
    "test_tupi = test_df[\"Tupi Antigo\"].tolist()\n",
    "test_portuguese_ref = test_df[\"Português\"].tolist()\n",
    "\n",
    "translations_ta_pt_zero = translate_batch(\n",
    "    model_zero_shot, \n",
    "    test_tupi, \n",
    "    source_lang=LANG_CODE,\n",
    "    target_lang=LANG_CODE\n",
    ")\n",
    "\n",
    "print(f\"\\nTraduzidas {len(translations_ta_pt_zero)} frases.\")\n",
    "\n",
    "# Calcular métricas\n",
    "metrics_ta_pt_zero = compute_all_metrics(translations_ta_pt_zero, test_portuguese_ref)\n",
    "\n",
    "print(\"\\n=== Métricas Zero-Shot TA → PT ===\")\n",
    "print(f\"  BLEU:  {metrics_ta_pt_zero['bleu']:.2f}\")\n",
    "print(f\"  chrF1: {metrics_ta_pt_zero['chrf1']:.2f}\")\n",
    "print(f\"  chrF3: {metrics_ta_pt_zero['chrf3']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237517e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar resultados zero-shot\n",
    "results_zero_shot = {\n",
    "    \"pt_to_ta\": {\n",
    "        \"bleu\": metrics_pt_ta_zero[\"bleu\"],\n",
    "        \"chrf1\": metrics_pt_ta_zero[\"chrf1\"],\n",
    "        \"chrf3\": metrics_pt_ta_zero[\"chrf3\"],\n",
    "        \"bleu_details\": metrics_pt_ta_zero[\"bleu_details\"]\n",
    "    },\n",
    "    \"ta_to_pt\": {\n",
    "        \"bleu\": metrics_ta_pt_zero[\"bleu\"],\n",
    "        \"chrf1\": metrics_ta_pt_zero[\"chrf1\"],\n",
    "        \"chrf3\": metrics_ta_pt_zero[\"chrf3\"],\n",
    "        \"bleu_details\": metrics_ta_pt_zero[\"bleu_details\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Salvar em arquivo JSON\n",
    "with open(os.path.join(CONFIG[\"results_dir\"], \"results_zero_shot.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_zero_shot, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Resultados zero-shot salvos em results/results_zero_shot.json\")\n",
    "\n",
    "# Salvar traduções\n",
    "os.makedirs(os.path.join(CONFIG[\"results_dir\"], \"outputs_zero_shot\"), exist_ok=True)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"source\": test_portuguese,\n",
    "    \"reference\": test_tupi_ref,\n",
    "    \"translation\": translations_pt_ta_zero\n",
    "}).to_csv(os.path.join(CONFIG[\"results_dir\"], \"outputs_zero_shot\", \"pt_to_ta.csv\"), index=False)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"source\": test_tupi,\n",
    "    \"reference\": test_portuguese_ref,\n",
    "    \"translation\": translations_ta_pt_zero\n",
    "}).to_csv(os.path.join(CONFIG[\"results_dir\"], \"outputs_zero_shot\", \"ta_to_pt.csv\"), index=False)\n",
    "\n",
    "print(\"Traduções salvas em results/outputs_zero_shot/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db067c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liberar memória do modelo zero-shot\n",
    "del model_zero_shot\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"Memória liberada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c740bd99",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Fine-Tuning (Few-Shot Learning)\n",
    "\n",
    "No regime **few-shot**, realizamos o ajuste fino (fine-tuning) do modelo mBART usando o corpus de treinamento.\n",
    "\n",
    "### Estratégia de Treinamento\n",
    "\n",
    "1. **LoRA (Low-Rank Adaptation)**: Utilizamos LoRA para reduzir o número de parâmetros treináveis, tornando o fine-tuning mais eficiente\n",
    "2. **Early Stopping**: Interrompemos o treinamento quando a perda de validação para de melhorar\n",
    "3. **Duas direções**: Treinamos modelos separados para PT→TA e TA→PT\n",
    "\n",
    "### Hiperparâmetros\n",
    "- Learning rate: 5e-5\n",
    "- Batch size: 4\n",
    "- Early stopping patience: 3 epochs\n",
    "- LoRA rank: 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4740457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainer(model, tokenizer, train_dataset, val_dataset, output_dir, direction):\n",
    "    \"\"\"\n",
    "    Cria um Seq2SeqTrainer configurado para o fine-tuning.\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo a ser treinado\n",
    "        tokenizer: Tokenizador\n",
    "        train_dataset: Dataset de treino tokenizado\n",
    "        val_dataset: Dataset de validação tokenizado\n",
    "        output_dir: Diretório para salvar checkpoints\n",
    "        direction: Direção da tradução ('pt_to_ta' ou 'ta_to_pt')\n",
    "        \n",
    "    Returns:\n",
    "        Trainer configurado\n",
    "    \"\"\"\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "    \n",
    "    def compute_metrics_trainer(eval_preds):\n",
    "        preds, labels = eval_preds\n",
    "        \n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "        \n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        \n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        # Para BLEU\n",
    "        refs = [[l] for l in decoded_labels]\n",
    "        bleu_result = bleu_metric.compute(predictions=decoded_preds, references=refs)\n",
    "        \n",
    "        return {\"bleu\": bleu_result[\"score\"]}\n",
    "    \n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=CONFIG[\"learning_rate\"],\n",
    "        per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
    "        per_device_eval_batch_size=CONFIG[\"batch_size\"],\n",
    "        num_train_epochs=CONFIG[\"num_epochs\"],\n",
    "        weight_decay=CONFIG[\"weight_decay\"],\n",
    "        warmup_steps=CONFIG[\"warmup_steps\"],\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"bleu\",\n",
    "        greater_is_better=True,\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=CONFIG[\"max_target_length\"],\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        logging_steps=50,\n",
    "        save_total_limit=2,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics_trainer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=CONFIG[\"early_stopping_patience\"])]\n",
    "    )\n",
    "    \n",
    "    return trainer\n",
    "\n",
    "print(\"Função de criação do trainer definida.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0509dd62",
   "metadata": {},
   "source": [
    "### 8.1 Fine-Tuning: Português → Tupi Antigo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f704e16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizar datasets para PT -> TA\n",
    "print(\"Tokenizando datasets para PT → TA...\")\n",
    "tokenized_train_pt_ta = dataset_dict[\"train\"].map(preprocess_pt_to_ta, batched=True)\n",
    "tokenized_val_pt_ta = dataset_dict[\"validation\"].map(preprocess_pt_to_ta, batched=True)\n",
    "\n",
    "print(f\"Treino: {len(tokenized_train_pt_ta)} exemplos\")\n",
    "print(f\"Validação: {len(tokenized_val_pt_ta)} exemplos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd9c8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar modelo base para PT -> TA\n",
    "print(\"Carregando modelo base para fine-tuning PT → TA...\")\n",
    "model_pt_ta = MBartForConditionalGeneration.from_pretrained(\n",
    "    CONFIG[\"model_checkpoint\"],\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "\n",
    "# Aplicar LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=CONFIG[\"lora_r\"],\n",
    "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "model_pt_ta = get_peft_model(model_pt_ta, lora_config)\n",
    "model_pt_ta.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628dd702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar modelo PT -> TA\n",
    "print(\"Iniciando treinamento PT → TA...\")\n",
    "trainer_pt_ta = create_trainer(\n",
    "    model=model_pt_ta,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_train_pt_ta,\n",
    "    val_dataset=tokenized_val_pt_ta,\n",
    "    output_dir=os.path.join(CONFIG[\"models_dir\"], \"pt_to_ta\"),\n",
    "    direction=\"pt_to_ta\"\n",
    ")\n",
    "\n",
    "train_result_pt_ta = trainer_pt_ta.train()\n",
    "\n",
    "print(\"\\n=== Treinamento PT → TA Concluído ===\")\n",
    "print(f\"Épocas: {train_result_pt_ta.metrics.get('epoch', 'N/A')}\")\n",
    "print(f\"Loss final: {train_result_pt_ta.metrics.get('train_loss', 'N/A'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fa2a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar modelo PT -> TA\n",
    "model_pt_ta.save_pretrained(os.path.join(CONFIG[\"models_dir\"], \"pt_to_ta_final\"))\n",
    "tokenizer.save_pretrained(os.path.join(CONFIG[\"models_dir\"], \"pt_to_ta_final\"))\n",
    "print(f\"Modelo PT → TA salvo em {CONFIG['models_dir']}/pt_to_ta_final/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe78c44",
   "metadata": {},
   "source": [
    "### 8.2 Fine-Tuning: Tupi Antigo → Português"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8fa293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liberar memória do modelo anterior\n",
    "del model_pt_ta, trainer_pt_ta\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Tokenizar datasets para TA -> PT\n",
    "print(\"Tokenizando datasets para TA → PT...\")\n",
    "tokenized_train_ta_pt = dataset_dict[\"train\"].map(preprocess_ta_to_pt, batched=True)\n",
    "tokenized_val_ta_pt = dataset_dict[\"validation\"].map(preprocess_ta_to_pt, batched=True)\n",
    "\n",
    "print(f\"Treino: {len(tokenized_train_ta_pt)} exemplos\")\n",
    "print(f\"Validação: {len(tokenized_val_ta_pt)} exemplos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7557fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar modelo base para TA -> PT\n",
    "print(\"Carregando modelo base para fine-tuning TA → PT...\")\n",
    "model_ta_pt = MBartForConditionalGeneration.from_pretrained(\n",
    "    CONFIG[\"model_checkpoint\"],\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "\n",
    "# Aplicar LoRA\n",
    "model_ta_pt = get_peft_model(model_ta_pt, lora_config)\n",
    "model_ta_pt.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0042f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar modelo TA -> PT\n",
    "print(\"Iniciando treinamento TA → PT...\")\n",
    "trainer_ta_pt = create_trainer(\n",
    "    model=model_ta_pt,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_train_ta_pt,\n",
    "    val_dataset=tokenized_val_ta_pt,\n",
    "    output_dir=os.path.join(CONFIG[\"models_dir\"], \"ta_to_pt\"),\n",
    "    direction=\"ta_to_pt\"\n",
    ")\n",
    "\n",
    "train_result_ta_pt = trainer_ta_pt.train()\n",
    "\n",
    "print(\"\\n=== Treinamento TA → PT Concluído ===\")\n",
    "print(f\"Épocas: {train_result_ta_pt.metrics.get('epoch', 'N/A')}\")\n",
    "print(f\"Loss final: {train_result_ta_pt.metrics.get('train_loss', 'N/A'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a735249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar modelo TA -> PT\n",
    "model_ta_pt.save_pretrained(os.path.join(CONFIG[\"models_dir\"], \"ta_to_pt_final\"))\n",
    "tokenizer.save_pretrained(os.path.join(CONFIG[\"models_dir\"], \"ta_to_pt_final\"))\n",
    "print(f\"Modelo TA → PT salvo em {CONFIG['models_dir']}/ta_to_pt_final/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7ef53e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Avaliação dos Modelos Fine-Tuned\n",
    "\n",
    "Agora avaliamos os modelos treinados no conjunto de teste, que nunca foi usado durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b45418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar modelos fine-tuned para avaliação\n",
    "from peft import PeftModel\n",
    "\n",
    "def load_finetuned_model(model_path):\n",
    "    \"\"\"Carrega um modelo fine-tuned com LoRA.\"\"\"\n",
    "    base_model = MBartForConditionalGeneration.from_pretrained(\n",
    "        CONFIG[\"model_checkpoint\"],\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Carregar modelos\n",
    "print(\"Carregando modelos fine-tuned...\")\n",
    "model_pt_ta_ft = load_finetuned_model(os.path.join(CONFIG[\"models_dir\"], \"pt_to_ta_final\"))\n",
    "print(\"  ✓ Modelo PT → TA carregado\")\n",
    "\n",
    "model_ta_pt_ft = load_finetuned_model(os.path.join(CONFIG[\"models_dir\"], \"ta_to_pt_final\"))\n",
    "print(\"  ✓ Modelo TA → PT carregado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ca6018",
   "metadata": {},
   "source": [
    "### 9.1 Avaliação PT → TA (Fine-Tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca39e3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar PT -> TA (fine-tuned)\n",
    "print(\"Traduzindo PT → TA (fine-tuned)...\")\n",
    "translations_pt_ta_ft = translate_batch(\n",
    "    model_pt_ta_ft,\n",
    "    test_portuguese,\n",
    "    source_lang=LANG_CODE,\n",
    "    target_lang=LANG_CODE\n",
    ")\n",
    "\n",
    "metrics_pt_ta_ft = compute_all_metrics(translations_pt_ta_ft, test_tupi_ref)\n",
    "\n",
    "print(\"\\n=== Métricas Fine-Tuned PT → TA ===\")\n",
    "print(f\"  BLEU:  {metrics_pt_ta_ft['bleu']:.2f}\")\n",
    "print(f\"  chrF1: {metrics_pt_ta_ft['chrf1']:.2f}\")\n",
    "print(f\"  chrF3: {metrics_pt_ta_ft['chrf3']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beb750a",
   "metadata": {},
   "source": [
    "### 9.2 Avaliação TA → PT (Fine-Tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2b7bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar TA -> PT (fine-tuned)\n",
    "print(\"Traduzindo TA → PT (fine-tuned)...\")\n",
    "translations_ta_pt_ft = translate_batch(\n",
    "    model_ta_pt_ft,\n",
    "    test_tupi,\n",
    "    source_lang=LANG_CODE,\n",
    "    target_lang=LANG_CODE\n",
    ")\n",
    "\n",
    "metrics_ta_pt_ft = compute_all_metrics(translations_ta_pt_ft, test_portuguese_ref)\n",
    "\n",
    "print(\"\\n=== Métricas Fine-Tuned TA → PT ===\")\n",
    "print(f\"  BLEU:  {metrics_ta_pt_ft['bleu']:.2f}\")\n",
    "print(f\"  chrF1: {metrics_ta_pt_ft['chrf1']:.2f}\")\n",
    "print(f\"  chrF3: {metrics_ta_pt_ft['chrf3']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac903b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar resultados few-shot\n",
    "results_few_shot = {\n",
    "    \"pt_to_ta\": {\n",
    "        \"bleu\": metrics_pt_ta_ft[\"bleu\"],\n",
    "        \"chrf1\": metrics_pt_ta_ft[\"chrf1\"],\n",
    "        \"chrf3\": metrics_pt_ta_ft[\"chrf3\"],\n",
    "        \"bleu_details\": metrics_pt_ta_ft[\"bleu_details\"]\n",
    "    },\n",
    "    \"ta_to_pt\": {\n",
    "        \"bleu\": metrics_ta_pt_ft[\"bleu\"],\n",
    "        \"chrf1\": metrics_ta_pt_ft[\"chrf1\"],\n",
    "        \"chrf3\": metrics_ta_pt_ft[\"chrf3\"],\n",
    "        \"bleu_details\": metrics_ta_pt_ft[\"bleu_details\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Salvar em arquivo JSON\n",
    "with open(os.path.join(CONFIG[\"results_dir\"], \"results_few_shot.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_few_shot, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Resultados few-shot salvos em results/results_few_shot.json\")\n",
    "\n",
    "# Salvar traduções\n",
    "os.makedirs(os.path.join(CONFIG[\"results_dir\"], \"outputs_few_shot\"), exist_ok=True)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"source\": test_portuguese,\n",
    "    \"reference\": test_tupi_ref,\n",
    "    \"translation\": translations_pt_ta_ft\n",
    "}).to_csv(os.path.join(CONFIG[\"results_dir\"], \"outputs_few_shot\", \"pt_to_ta.csv\"), index=False)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"source\": test_tupi,\n",
    "    \"reference\": test_portuguese_ref,\n",
    "    \"translation\": translations_ta_pt_ft\n",
    "}).to_csv(os.path.join(CONFIG[\"results_dir\"], \"outputs_few_shot\", \"ta_to_pt.csv\"), index=False)\n",
    "\n",
    "print(\"Traduções salvas em results/outputs_few_shot/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec88ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Comparação: Zero-Shot vs Fine-Tuned\n",
    "\n",
    "### Matriz Comparativa de Métricas\n",
    "\n",
    "Comparamos os resultados obtidos nos dois regimes (zero-shot e few-shot) para ambas as direções de tradução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc786c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar matriz comparativa\n",
    "comparison_data = {\n",
    "    \"Direção\": [\"PT → TA\", \"PT → TA\", \"TA → PT\", \"TA → PT\"],\n",
    "    \"Modo\": [\"Zero-Shot\", \"Fine-Tuned\", \"Zero-Shot\", \"Fine-Tuned\"],\n",
    "    \"BLEU\": [\n",
    "        metrics_pt_ta_zero[\"bleu\"],\n",
    "        metrics_pt_ta_ft[\"bleu\"],\n",
    "        metrics_ta_pt_zero[\"bleu\"],\n",
    "        metrics_ta_pt_ft[\"bleu\"]\n",
    "    ],\n",
    "    \"chrF1\": [\n",
    "        metrics_pt_ta_zero[\"chrf1\"],\n",
    "        metrics_pt_ta_ft[\"chrf1\"],\n",
    "        metrics_ta_pt_zero[\"chrf1\"],\n",
    "        metrics_ta_pt_ft[\"chrf1\"]\n",
    "    ],\n",
    "    \"chrF3\": [\n",
    "        metrics_pt_ta_zero[\"chrf3\"],\n",
    "        metrics_pt_ta_ft[\"chrf3\"],\n",
    "        metrics_ta_pt_zero[\"chrf3\"],\n",
    "        metrics_ta_pt_ft[\"chrf3\"]\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"=== Matriz Comparativa de Métricas ===\\n\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Calcular melhorias\n",
    "print(\"\\n\\n=== Melhoria com Fine-Tuning ===\")\n",
    "print(f\"\\nPT → TA:\")\n",
    "print(f\"  BLEU:  {metrics_pt_ta_ft['bleu'] - metrics_pt_ta_zero['bleu']:+.2f} pontos\")\n",
    "print(f\"  chrF1: {metrics_pt_ta_ft['chrf1'] - metrics_pt_ta_zero['chrf1']:+.2f} pontos\")\n",
    "print(f\"  chrF3: {metrics_pt_ta_ft['chrf3'] - metrics_pt_ta_zero['chrf3']:+.2f} pontos\")\n",
    "\n",
    "print(f\"\\nTA → PT:\")\n",
    "print(f\"  BLEU:  {metrics_ta_pt_ft['bleu'] - metrics_ta_pt_zero['bleu']:+.2f} pontos\")\n",
    "print(f\"  chrF1: {metrics_ta_pt_ft['chrf1'] - metrics_ta_pt_zero['chrf1']:+.2f} pontos\")\n",
    "print(f\"  chrF3: {metrics_ta_pt_ft['chrf3'] - metrics_ta_pt_zero['chrf3']:+.2f} pontos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d535f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização gráfica\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    metrics_names = [\"BLEU\", \"chrF1\", \"chrF3\"]\n",
    "    x = np.arange(2)\n",
    "    width = 0.35\n",
    "    \n",
    "    for idx, metric in enumerate(metrics_names):\n",
    "        ax = axes[idx]\n",
    "        zero_shot = [metrics_pt_ta_zero[metric.lower()], metrics_ta_pt_zero[metric.lower()]]\n",
    "        fine_tuned = [metrics_pt_ta_ft[metric.lower()], metrics_ta_pt_ft[metric.lower()]]\n",
    "        \n",
    "        bars1 = ax.bar(x - width/2, zero_shot, width, label='Zero-Shot', color='#ff7f0e')\n",
    "        bars2 = ax.bar(x + width/2, fine_tuned, width, label='Fine-Tuned', color='#1f77b4')\n",
    "        \n",
    "        ax.set_xlabel('Direção')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.set_title(f'{metric}')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(['PT → TA', 'TA → PT'])\n",
    "        ax.legend()\n",
    "        ax.set_ylim(0, 100)\n",
    "        \n",
    "        # Adicionar valores nas barras\n",
    "        for bar in bars1 + bars2:\n",
    "            height = bar.get_height()\n",
    "            ax.annotate(f'{height:.1f}',\n",
    "                       xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                       xytext=(0, 3),\n",
    "                       textcoords=\"offset points\",\n",
    "                       ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    plt.suptitle('Comparação: Zero-Shot vs Fine-Tuned', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CONFIG[\"results_dir\"], \"comparison_chart.png\"), dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"\\nGráfico salvo em {CONFIG['results_dir']}/comparison_chart.png\")\n",
    "except ImportError:\n",
    "    print(\"matplotlib não disponível para visualização gráfica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bc6ac3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Exemplos Qualitativos\n",
    "\n",
    "Analisamos algumas traduções para entender qualitativamente o desempenho dos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e527275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplos qualitativos PT -> TA\n",
    "print(\"=\" * 80)\n",
    "print(\"EXEMPLOS QUALITATIVOS: Português → Tupi Antigo\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "n_examples = min(10, len(test_portuguese))\n",
    "for i in range(n_examples):\n",
    "    print(f\"\\n--- Exemplo {i+1} ---\")\n",
    "    print(f\"📝 Fonte (PT):      {test_portuguese[i]}\")\n",
    "    print(f\"✅ Referência (TA): {test_tupi_ref[i]}\")\n",
    "    print(f\"🔴 Zero-Shot:       {translations_pt_ta_zero[i]}\")\n",
    "    print(f\"🟢 Fine-Tuned:      {translations_pt_ta_ft[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff541535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplos qualitativos TA -> PT\n",
    "print(\"=\" * 80)\n",
    "print(\"EXEMPLOS QUALITATIVOS: Tupi Antigo → Português\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(n_examples):\n",
    "    print(f\"\\n--- Exemplo {i+1} ---\")\n",
    "    print(f\"📝 Fonte (TA):      {test_tupi[i]}\")\n",
    "    print(f\"✅ Referência (PT): {test_portuguese_ref[i]}\")\n",
    "    print(f\"🔴 Zero-Shot:       {translations_ta_pt_zero[i]}\")\n",
    "    print(f\"🟢 Fine-Tuned:      {translations_ta_pt_ft[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dc9314",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Discussão e Limitações\n",
    "\n",
    "### 12.1 Análise dos Resultados\n",
    "\n",
    "#### Zero-Shot\n",
    "- O modelo mBART, apesar de multilíngue, não possui conhecimento prévio do Tupi Antigo\n",
    "- As traduções zero-shot tendem a ser em português ou em outros idiomas do vocabulário do modelo\n",
    "- As métricas baixas refletem essa limitação fundamental\n",
    "\n",
    "#### Fine-Tuned\n",
    "- O fine-tuning com LoRA permite adaptar o modelo ao par linguístico Português-Tupi Antigo\n",
    "- Mesmo com um corpus pequeno, observamos melhoria significativa nas métricas\n",
    "- O modelo aprende padrões específicos da língua Tupi, incluindo sua morfologia\n",
    "\n",
    "### 12.2 Limitações\n",
    "\n",
    "1. **Tamanho do Corpus**: Corpora de baixo recurso limitam o aprendizado do modelo\n",
    "\n",
    "2. **Tokenização**: O tokenizador do mBART não foi otimizado para Tupi Antigo, podendo segmentar palavras de forma subótima\n",
    "\n",
    "3. **Código de Idioma**: Usamos pt_XX como proxy, o que não é ideal\n",
    "\n",
    "4. **Variação Ortográfica**: O Tupi Antigo possui variações históricas que podem confundir o modelo\n",
    "\n",
    "5. **Avaliação Automática**: Métricas como BLEU podem não capturar adequadamente a qualidade semântica\n",
    "\n",
    "### 12.3 Trabalhos Futuros\n",
    "\n",
    "1. Expandir o corpus paralelo\n",
    "2. Desenvolver um tokenizador específico para Tupi Antigo\n",
    "3. Explorar técnicas de data augmentation\n",
    "4. Avaliar outros modelos (NLLB-200, mT5)\n",
    "5. Realizar avaliação humana das traduções"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7d6672",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. Conclusão\n",
    "\n",
    "Neste EP, implementamos e avaliamos tradutores automáticos para o par linguístico **Português ↔ Tupi Antigo** em dois regimes:\n",
    "\n",
    "### Principais Resultados\n",
    "\n",
    "1. **Zero-Shot**: O modelo mBART pré-treinado não consegue traduzir adequadamente para/de Tupi Antigo sem fine-tuning, confirmando a necessidade de adaptação para línguas de baixo recurso.\n",
    "\n",
    "2. **Fine-Tuned**: Com fine-tuning usando LoRA, observamos melhorias significativas em todas as métricas, demonstrando que mesmo corpora pequenos podem ser úteis para adaptar modelos multilíngues.\n",
    "\n",
    "3. **Direção da Tradução**: A tradução TA → PT tende a ter melhores resultados, possivelmente porque o português é uma língua bem representada no modelo base.\n",
    "\n",
    "### Arquivos Gerados\n",
    "\n",
    "- `results/results_zero_shot.json`: Métricas do regime zero-shot\n",
    "- `results/results_few_shot.json`: Métricas do regime few-shot\n",
    "- `results/outputs_zero_shot/`: Traduções geradas (zero-shot)\n",
    "- `results/outputs_few_shot/`: Traduções geradas (fine-tuned)\n",
    "- `models/pt_to_ta_final/`: Modelo fine-tuned PT → TA\n",
    "- `models/ta_to_pt_final/`: Modelo fine-tuned TA → PT\n",
    "- `data/train.csv`, `data/val.csv`, `data/test.csv`: Divisões do corpus\n",
    "\n",
    "---\n",
    "\n",
    "**MAC0508 — Introdução ao Processamento de Língua Natural**  \n",
    "**EP2 — Tradução Automática de Baixo Recurso**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8319351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo final dos resultados\n",
    "print(\"=\" * 60)\n",
    "print(\"RESUMO FINAL DOS RESULTADOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n📊 MÉTRICAS FINAIS\\n\")\n",
    "print(f\"{'Cenário':<25} {'BLEU':>10} {'chrF1':>10} {'chrF3':>10}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'PT→TA Zero-Shot':<25} {metrics_pt_ta_zero['bleu']:>10.2f} {metrics_pt_ta_zero['chrf1']:>10.2f} {metrics_pt_ta_zero['chrf3']:>10.2f}\")\n",
    "print(f\"{'PT→TA Fine-Tuned':<25} {metrics_pt_ta_ft['bleu']:>10.2f} {metrics_pt_ta_ft['chrf1']:>10.2f} {metrics_pt_ta_ft['chrf3']:>10.2f}\")\n",
    "print(f\"{'TA→PT Zero-Shot':<25} {metrics_ta_pt_zero['bleu']:>10.2f} {metrics_ta_pt_zero['chrf1']:>10.2f} {metrics_ta_pt_zero['chrf3']:>10.2f}\")\n",
    "print(f\"{'TA→PT Fine-Tuned':<25} {metrics_ta_pt_ft['bleu']:>10.2f} {metrics_ta_pt_ft['chrf1']:>10.2f} {metrics_ta_pt_ft['chrf3']:>10.2f}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n✅ Notebook executado com sucesso!\")\n",
    "print(f\"📁 Resultados salvos em: {CONFIG['results_dir']}/\")\n",
    "print(f\"🤖 Modelos salvos em: {CONFIG['models_dir']}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
